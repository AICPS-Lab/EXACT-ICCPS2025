{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fewshot Semantic Segmentation\n",
    "\"\"\"\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# make a dummy Encoder class for pytorch:\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, pretrained_path):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.pretrained_path = pretrained_path\n",
    "        self.in_channels = in_channels\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2, ceil_mode=True),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2, ceil_mode=True),\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2, ceil_mode=True),\n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2, ceil_mode=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2, ceil_mode=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "class FewShotSeg(nn.Module):\n",
    "    \"\"\"\n",
    "    Fewshot Segmentation model\n",
    "\n",
    "    Args:\n",
    "        in_channels:\n",
    "            number of input channels\n",
    "        pretrained_path:\n",
    "            path of the model for initialization\n",
    "        cfg:\n",
    "            model configurations\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, pretrained_path=None, cfg=None):\n",
    "        super().__init__()\n",
    "        self.pretrained_path = pretrained_path\n",
    "        self.config = cfg or {'align': True}\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(OrderedDict([\n",
    "            ('backbone', Encoder(in_channels, self.pretrained_path)),]))\n",
    "\n",
    "\n",
    "    def forward(self, supp_imgs, fore_mask, back_mask, qry_imgs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            supp_imgs: support images\n",
    "                way x shot x [B x 3 x H x W], list of lists of tensors\n",
    "            fore_mask: foreground masks for support images\n",
    "                way x shot x [B x H x W], list of lists of tensors\n",
    "            back_mask: background masks for support images\n",
    "                way x shot x [B x H x W], list of lists of tensors\n",
    "            qry_imgs: query images\n",
    "                N x [B x 3 x H x W], list of tensors\n",
    "        \"\"\"\n",
    "        n_ways = len(supp_imgs)\n",
    "        n_shots = len(supp_imgs[0])\n",
    "        n_queries = len(qry_imgs)\n",
    "        batch_size = supp_imgs[0][0].shape[0]\n",
    "        img_size = supp_imgs[0][0].shape[-2:]\n",
    "        print('n_ways:', n_ways, 'n_shots:', n_shots, 'n_queries:', n_queries, 'batch_size:', batch_size, 'img_size:', img_size)\n",
    "        ###### Extract features ######\n",
    "        print('supp_imgs shape:', supp_imgs[0][0].shape)\n",
    "        imgs_concat = torch.cat([torch.cat(way, dim=0) for way in supp_imgs]\n",
    "                                + [torch.cat(qry_imgs, dim=0),], dim=0)\n",
    "        print('after concat', imgs_concat.shape, 'which is (Wa*Sh + N) x B x 3 x H x W, Wa =', n_ways, 'Sh =', n_shots, 'N =', n_queries, 'B =', batch_size)\n",
    "        img_fts = self.encoder(imgs_concat)\n",
    "        print('after go through encoder', img_fts.shape)\n",
    "        fts_size = img_fts.shape[-2:]\n",
    "        print('fts_size:', fts_size)\n",
    "\n",
    "        supp_fts = img_fts[:n_ways * n_shots * batch_size].view(\n",
    "            n_ways, n_shots, batch_size, -1, *fts_size)  # Wa x Sh x B x C x H' x W'\n",
    "        print('supp_fts shape:', supp_fts.shape)\n",
    "        qry_fts = img_fts[n_ways * n_shots * batch_size:].view(\n",
    "            n_queries, batch_size, -1, *fts_size)   # N x B x C x H' x W'\n",
    "        print('qry_fts shape:', qry_fts.shape)\n",
    "        fore_mask = torch.stack([torch.stack(way, dim=0)\n",
    "                                 for way in fore_mask], dim=0)  # Wa x Sh x B x H x W\n",
    "        print('fore_mask shape:', fore_mask.shape)\n",
    "        back_mask = torch.stack([torch.stack(way, dim=0)\n",
    "                                 for way in back_mask], dim=0)  # Wa x Sh x B x H x W\n",
    "        print('back_mask shape:', back_mask.shape)\n",
    "        \n",
    "\n",
    "        ###### Compute loss ######\n",
    "        align_loss = 0\n",
    "        outputs = []\n",
    "        for epi in range(batch_size):\n",
    "            ###### Extract prototype ######\n",
    "            supp_fg_fts = [[self.getFeatures(supp_fts[way, shot, [epi]],\n",
    "                                             fore_mask[way, shot, [epi]])\n",
    "                            for shot in range(n_shots)] for way in range(n_ways)]\n",
    "            supp_bg_fts = [[self.getFeatures(supp_fts[way, shot, [epi]],\n",
    "                                             back_mask[way, shot, [epi]])\n",
    "                            for shot in range(n_shots)] for way in range(n_ways)]\n",
    "            print('supp_fg_fts shape:', len(supp_fg_fts), len(supp_fg_fts[0]), supp_fg_fts[0][0].shape)\n",
    "            ###### Obtain the prototypes######\n",
    "            fg_prototypes, bg_prototype = self.getPrototype(supp_fg_fts, supp_bg_fts)\n",
    "            print('fg_prototypes shape:', len(fg_prototypes), fg_prototypes[0].shape)\n",
    "            print('bg_prototype shape:', bg_prototype.shape)\n",
    "\n",
    "            ###### Compute the distance ######\n",
    "            prototypes = [bg_prototype,] + fg_prototypes\n",
    "            print('prototypes shape:', len(prototypes), prototypes[0].shape)\n",
    "            dist = [self.calDist(qry_fts[:, epi], prototype) for prototype in prototypes]\n",
    "            print('dist shape:', len(dist), dist[0].shape)\n",
    "            pred = torch.stack(dist, dim=1)  # N x (1 + Wa) x H' x W'\n",
    "            print('pred shape:', pred.shape)\n",
    "            outputs.append(F.interpolate(pred, size=img_size, mode='bilinear'))\n",
    "            print('UPSAMPLING: outputs shape:', len(outputs), outputs[0].shape)\n",
    "\n",
    "            ###### Prototype alignment loss ######\n",
    "            if self.config['align'] :\n",
    "                align_loss_epi = self.alignLoss(qry_fts[:, epi], pred, supp_fts[:, :, epi],\n",
    "                                                fore_mask[:, :, epi], back_mask[:, :, epi])\n",
    "                align_loss += align_loss_epi\n",
    "\n",
    "        output = torch.stack(outputs, dim=1)  # N x B x (1 + Wa) x H x W\n",
    "        print('FINAL OUTPUT shape N x B x (1 + Wa) x H x W:', output.shape)\n",
    "        output = output.view(-1, *output.shape[2:])\n",
    "        print(output.shape)\n",
    "        return output, align_loss / batch_size\n",
    "\n",
    "\n",
    "    def calDist(self, fts, prototype, scaler=20):\n",
    "        \"\"\"\n",
    "        Calculate the distance between features and prototypes\n",
    "\n",
    "        Args:\n",
    "            fts: input features\n",
    "                expect shape: N x C x H x W\n",
    "            prototype: prototype of one semantic class\n",
    "                expect shape: 1 x C\n",
    "        \"\"\"\n",
    "        print('CALDIST fts shape:', fts.shape, 'prototype shape:', prototype.shape)\n",
    "        dist = F.cosine_similarity(fts, prototype[..., None, None], dim=1) * scaler\n",
    "        return dist\n",
    "\n",
    "\n",
    "    def getFeatures(self, fts, mask):\n",
    "        \"\"\"\n",
    "        Extract foreground and background features via masked average pooling\n",
    "\n",
    "        Args:\n",
    "            fts: input features, expect shape: 1 x C x H' x W'\n",
    "            mask: binary mask, expect shape: 1 x H x W\n",
    "        \"\"\"\n",
    "        fts = F.interpolate(fts, size=mask.shape[-2:], mode='bilinear')\n",
    "        masked_fts = torch.sum(fts * mask[None, ...], dim=(2, 3)) \\\n",
    "            / (mask[None, ...].sum(dim=(2, 3)) + 1e-5) # 1 x C\n",
    "        return masked_fts\n",
    "\n",
    "\n",
    "    def getPrototype(self, fg_fts, bg_fts):\n",
    "        \"\"\"\n",
    "        Average the features to obtain the prototype\n",
    "\n",
    "        Args:\n",
    "            fg_fts: lists of list of foreground features for each way/shot\n",
    "                expect shape: Wa x Sh x [1 x C]\n",
    "            bg_fts: lists of list of background features for each way/shot\n",
    "                expect shape: Wa x Sh x [1 x C]\n",
    "        \"\"\"\n",
    "        n_ways, n_shots = len(fg_fts), len(fg_fts[0])\n",
    "        fg_prototypes = [sum(way) / n_shots for way in fg_fts]\n",
    "        bg_prototype = sum([sum(way) / n_shots for way in bg_fts]) / n_ways\n",
    "        return fg_prototypes, bg_prototype\n",
    "\n",
    "\n",
    "    def alignLoss(self, qry_fts, pred, supp_fts, fore_mask, back_mask):\n",
    "        \"\"\"\n",
    "        Compute the loss for the prototype alignment branch\n",
    "\n",
    "        Args:\n",
    "            qry_fts: embedding features for query images\n",
    "                expect shape: N x C x H' x W'\n",
    "            pred: predicted segmentation score\n",
    "                expect shape: N x (1 + Wa) x H x W\n",
    "            supp_fts: embedding features for support images\n",
    "                expect shape: Wa x Sh x C x H' x W'\n",
    "            fore_mask: foreground masks for support images\n",
    "                expect shape: way x shot x H x W\n",
    "            back_mask: background masks for support images\n",
    "                expect shape: way x shot x H x W\n",
    "        \"\"\"\n",
    "        n_ways, n_shots = len(fore_mask), len(fore_mask[0])\n",
    "\n",
    "        # Mask and get query prototype\n",
    "        pred_mask = pred.argmax(dim=1, keepdim=True)  # N x 1 x H' x W'\n",
    "        binary_masks = [pred_mask == i for i in range(1 + n_ways)]\n",
    "        skip_ways = [i for i in range(n_ways) if binary_masks[i + 1].sum() == 0]\n",
    "        pred_mask = torch.stack(binary_masks, dim=1).float()  # N x (1 + Wa) x 1 x H' x W'\n",
    "        qry_prototypes = torch.sum(qry_fts.unsqueeze(1) * pred_mask, dim=(0, 3, 4))\n",
    "        qry_prototypes = qry_prototypes / (pred_mask.sum((0, 3, 4)) + 1e-5)  # (1 + Wa) x C\n",
    "        print('qry_prototypes shape:', qry_prototypes.shape)\n",
    "        # Compute the support loss\n",
    "        loss = 0\n",
    "        for way in range(n_ways):\n",
    "            if way in skip_ways:\n",
    "                continue\n",
    "            # Get the query prototypes\n",
    "            prototypes = [qry_prototypes[[0]], qry_prototypes[[way + 1]]]\n",
    "            for shot in range(n_shots):\n",
    "                img_fts = supp_fts[way, [shot]]\n",
    "                supp_dist = [self.calDist(img_fts, prototype) for prototype in prototypes]\n",
    "                supp_pred = torch.stack(supp_dist, dim=1)\n",
    "                supp_pred = F.interpolate(supp_pred, size=fore_mask.shape[-2:],\n",
    "                                          mode='bilinear')\n",
    "                # Construct the support Ground-Truth segmentation\n",
    "                supp_label = torch.full_like(fore_mask[way, shot], 255,\n",
    "                                             device=img_fts.device).long()\n",
    "                supp_label[fore_mask[way, shot] == 1] = 1\n",
    "                supp_label[back_mask[way, shot] == 1] = 0\n",
    "                # Compute Loss\n",
    "                loss = loss + F.cross_entropy(\n",
    "                    supp_pred, supp_label[None, ...], ignore_index=255) / n_shots / n_ways\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotSeg(\n",
       "  (encoder): Sequential(\n",
       "    (backbone): Encoder(\n",
       "      (encoder): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "        (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (8): ReLU(inplace=True)\n",
       "        (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "        (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (11): ReLU(inplace=True)\n",
       "        (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (13): ReLU(inplace=True)\n",
       "        (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "        (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (16): ReLU(inplace=True)\n",
       "        (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (18): ReLU(inplace=True)\n",
       "        (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "        (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (21): ReLU(inplace=True)\n",
       "        (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (23): ReLU(inplace=True)\n",
       "        (24): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dummy model for testing\n",
    "model = FewShotSeg()\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy input\n",
    "supp_imgs = [[torch.rand(2, 3, 125, 125) for _ in range(5)] for _ in range(5)]\n",
    "fore_mask = [[torch.rand(2, 125, 125) for _ in range(5)] for _ in range(5)]\n",
    "back_mask = [[torch.rand(2, 125, 125) for _ in range(5)] for _ in range(5)] # 5 ways, 5 shots, 2 batch size\n",
    "qry_imgs = [torch.rand(2, 3, 125, 125) for _ in range(5)] # 5 images, 2 batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_ways: 5 n_shots: 5 n_queries: 5 batch_size: 2 img_size: torch.Size([125, 125])\n",
      "supp_imgs shape: torch.Size([2, 3, 125, 125])\n",
      "after concat torch.Size([60, 3, 125, 125]) which is (Wa*Sh + N) x B x 3 x H x W, Wa = 5 Sh = 5 N = 5 B = 2\n",
      "after go through encoder torch.Size([60, 512, 4, 4])\n",
      "fts_size: torch.Size([4, 4])\n",
      "supp_fts shape: torch.Size([5, 5, 2, 512, 4, 4])\n",
      "qry_fts shape: torch.Size([5, 2, 512, 4, 4])\n",
      "fore_mask shape: torch.Size([5, 5, 2, 125, 125])\n",
      "back_mask shape: torch.Size([5, 5, 2, 125, 125])\n",
      "supp_fg_fts shape: 5 5 torch.Size([1, 512])\n",
      "fg_prototypes shape: 5 torch.Size([1, 512])\n",
      "bg_prototype shape: torch.Size([1, 512])\n",
      "prototypes shape: 6 torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([5, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([5, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([5, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([5, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([5, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([5, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "dist shape: 6 torch.Size([5, 4, 4])\n",
      "pred shape: torch.Size([5, 6, 4, 4])\n",
      "UPSAMPLING: outputs shape: 1 torch.Size([5, 6, 125, 125])\n",
      "qry_prototypes shape: torch.Size([6, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "supp_fg_fts shape: 5 5 torch.Size([1, 512])\n",
      "fg_prototypes shape: 5 torch.Size([1, 512])\n",
      "bg_prototype shape: torch.Size([1, 512])\n",
      "prototypes shape: 6 torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([5, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([5, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([5, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([5, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([5, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([5, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "dist shape: 6 torch.Size([5, 4, 4])\n",
      "pred shape: torch.Size([5, 6, 4, 4])\n",
      "UPSAMPLING: outputs shape: 2 torch.Size([5, 6, 125, 125])\n",
      "qry_prototypes shape: torch.Size([6, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "CALDIST fts shape: torch.Size([1, 512, 4, 4]) prototype shape: torch.Size([1, 512])\n",
      "FINAL OUTPUT shape N x B x (1 + Wa) x H x W: torch.Size([5, 2, 6, 125, 125])\n",
      "torch.Size([10, 6, 125, 125])\n"
     ]
    }
   ],
   "source": [
    "# run the model\n",
    "output, align_loss = model(supp_imgs, fore_mask, back_mask, qry_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 6, 125, 125]),\n",
       " tensor(nan, grad_fn=<DivBackward0>),\n",
       " array([ 10, 125, 125]))"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "output.shape, align_loss, np.array(output.argmax(dim=1).cpu().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild the whole thing to work with itme series: original input is 5x5x2x3x125x125 -> 5x5x2x6x100\n",
    "# 5x5x2x3x125x125 -> 5x5x2x6x100, mask would be 5x5x2x125x125 -> 5x5x2x100\n",
    "\n",
    "# create dummy encoder for time series (6x100)\n",
    "# class time_Encoder(nn.Module):\n",
    "#     # input could be Batchx6x100, just use very simple network with conv1d:\n",
    "#     def __init__(self, in_channels, pretrained_path):\n",
    "#         super(time_Encoder, self).__init__()\n",
    "#         self.pretrained_path = pretrained_path\n",
    "#         self.in_channels = in_channels\n",
    "#         self.encoder = nn.Sequential(\n",
    "#             nn.Linear(6, 512),\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = x.transpose(1, 2)\n",
    "#         x = self.encoder(x)\n",
    "#         x = x.transpose(1, 2)\n",
    "#         x = nn.Linear(100, 2)(x)\n",
    "#         return  x\n",
    "    \n",
    "class time_Encoder(nn.Module):\n",
    "    # input could be Batchx6x100, just use very simple network with conv1d:\n",
    "    def __init__(self, in_channels, pretrained_path):\n",
    "        super(time_Encoder, self).__init__()\n",
    "        self.pretrained_path = pretrained_path\n",
    "        self.in_channels = in_channels\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(6, 512),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.encoder(x).transpose(1, 2)\n",
    "        return  x\n",
    "# create dummy model for testing\n",
    "class time_FewShotSeg(nn.Module):\n",
    "    \"\"\"\n",
    "    Fewshot Segmentation model\n",
    "\n",
    "    Args:\n",
    "        in_channels:\n",
    "            number of input channels\n",
    "        pretrained_path:\n",
    "            path of the model for initialization\n",
    "        cfg:\n",
    "            model configurations\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=6, pretrained_path=None, cfg=None):\n",
    "        super().__init__()\n",
    "        self.pretrained_path = pretrained_path\n",
    "        self.config = cfg or {'align': True}\n",
    "        \n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(OrderedDict([\n",
    "            ('backbone', time_Encoder(in_channels, self.pretrained_path)),]))\n",
    "\n",
    "    def forward(self, supp_imgs, fore_mask, back_mask, qry_imgs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            supp_imgs: support images\n",
    "                way x shot x [B x 6 x 100], list of lists of tensors\n",
    "            fore_mask: foreground masks for support images\n",
    "                way x shot x [B x 100], list of lists of tensors\n",
    "            back_mask: background masks for support images\n",
    "                way x shot x [B x 100], list of lists of tensors\n",
    "            qry_imgs: query images\n",
    "                N x [B x 6 x 100], list of tensors\n",
    "        \"\"\"\n",
    "        n_ways = len(supp_imgs)\n",
    "        n_shots = len(supp_imgs[0])\n",
    "        n_queries = len(qry_imgs)\n",
    "        batch_size = supp_imgs[0][0].shape[0]\n",
    "        img_size = supp_imgs[0][0].shape[-1:]\n",
    "        print('n_ways:', n_ways, 'n_shots:', n_shots, 'n_queries:', n_queries, 'batch_size:', batch_size, 'img_size:', img_size)\n",
    "        ###### Extract features ######\n",
    "        print('supp_imgs shape:', supp_imgs[0][0].shape)\n",
    "        imgs_concat = torch.cat([torch.cat(way, dim=0) for way in supp_imgs]\n",
    "                                + [torch.cat(qry_imgs, dim=0),], dim=0)\n",
    "        print('after concat', imgs_concat.shape, 'which is (Wa*Sh + N) x B x 6 x 100, Wa =', n_ways, 'Sh =', n_shots, 'N =', n_queries, 'B =', batch_size)\n",
    "        print(imgs_concat.shape)\n",
    "        img_fts = self.encoder(imgs_concat)\n",
    "        print('after go through encoder', img_fts.shape)\n",
    "        fts_size = img_fts.shape[-1:] # this is the length, aka 100 (Time series length)\n",
    "        print('fts_size:', fts_size)\n",
    "        \n",
    "        supp_fts = img_fts[:n_ways * n_shots * batch_size].view(\n",
    "            n_ways, n_shots, batch_size, -1, *fts_size)\n",
    "        print('supp_fts shape:', supp_fts.shape)\n",
    "        qry_fts = img_fts[n_ways * n_shots * batch_size:].view(\n",
    "            n_queries, batch_size, -1, *fts_size)\n",
    "        print('qry_fts shape:', qry_fts.shape)\n",
    "        fore_mask = torch.stack([torch.stack(way, dim=0)\n",
    "                                 for way in fore_mask], dim=0)\n",
    "        print('fore_mask shape:', fore_mask.shape)\n",
    "        back_mask = torch.stack([torch.stack(way, dim=0)\n",
    "                                 for way in back_mask], dim=0)\n",
    "        print('back_mask shape:', back_mask.shape)\n",
    "        \n",
    "\n",
    "        ###### Compute loss ######\n",
    "        align_loss = 0\n",
    "        outputs = []\n",
    "        for epi in range(batch_size):\n",
    "            ###### Extract prototype ######\n",
    "            supp_fg_fts = [[self.getFeatures(supp_fts[way, shot, [epi]],\n",
    "                                             fore_mask[way, shot, [epi]])\n",
    "                            for shot in range(n_shots)] for way in range(n_ways)] # 1 x C (channel embedding)\n",
    "            supp_bg_fts = [[self.getFeatures(supp_fts[way, shot, [epi]],\n",
    "                                             back_mask[way, shot, [epi]])\n",
    "                            for shot in range(n_shots)] for way in range(n_ways)] # 1 x C (channel embedding)\n",
    "            ###### Obtain the prototypes######\n",
    "            fg_prototypes, bg_prototype = self.getPrototype(supp_fg_fts, supp_bg_fts) \n",
    "            print('fg_prototypes shape:', len(fg_prototypes), fg_prototypes[0].shape)\n",
    "            print('bg_prototype shape:', bg_prototype.shape)\n",
    "\n",
    "            ###### Compute the distance ######\n",
    "            prototypes = [bg_prototype,] + fg_prototypes\n",
    "            print('prototypes shape:', len(prototypes), prototypes[0].shape)\n",
    "            dist = [self.calDist(qry_fts[:, epi], prototype) for prototype in prototypes]\n",
    "            print('dist shape:', len(dist), dist[0].shape)\n",
    "            pred = torch.stack(dist, dim=1)\n",
    "            print('pred shape:', pred.shape)\n",
    "            outputs.append(F.interpolate(pred, size=img_size, mode='linear'))\n",
    "            print('upsampling', F.interpolate(pred, size=img_size, mode='linear').shape)\n",
    "            print('outputs shape:', len(outputs), outputs[0].shape)\n",
    "\n",
    "            ###### Prototype alignment loss ######\n",
    "            if self.config['align']:\n",
    "                align_loss_epi = self.alignLoss(qry_fts[:, epi], pred, supp_fts[:, :, epi],\n",
    "                                                fore_mask[:, :, epi], back_mask[:, :, epi])\n",
    "                align_loss += align_loss_epi\n",
    "        print('outputs shape:', len(outputs), outputs[0].shape)\n",
    "        output = torch.stack(outputs, dim=1)  # N x B x (1 + Wa) x H x W\n",
    "        output = output.view(-1, *output.shape[2:])\n",
    "        print(output.shape)\n",
    "        return output, align_loss / batch_size\n",
    "        \n",
    "    def calDist(self, fts, prototype, scaler=20):\n",
    "        \"\"\"\n",
    "        Calculate the distance between features and prototypes\n",
    "\n",
    "        Args:\n",
    "            fts: input features\n",
    "                expect shape: N x C x H\n",
    "            prototype: prototype of one semantic class\n",
    "                expect shape: 1 x C\n",
    "        \"\"\"\n",
    "        # print('CALDIST fts shape:', fts.shape, 'prototype shape:', prototype.shape)\n",
    "        dist = F.cosine_similarity(fts, prototype[..., None], dim=1) * scaler\n",
    "        return dist\n",
    "        \n",
    "    def getFeatures(self, fts, mask):\n",
    "        \"\"\"\n",
    "        Extract foreground and background features via masked average pooling\n",
    "\n",
    "        Args:\n",
    "            fts: input features, expect shape: 1 x C x H'\n",
    "            mask: binary mask, expect shape: 1 x H\n",
    "            \n",
    "        originally: \n",
    "        fts = F.interpolate(fts, size=mask.shape[-2:], mode='bilinear')\n",
    "        # IndexError: Dimension out of range (expected to be in range of [-3, 2], but got 3)\n",
    "        masked_fts = torch.sum(fts * mask[None, ...], dim=(2, 3)) \\\n",
    "            / (mask[None, ...].sum(dim=(2)) + 1e-5)\n",
    "        return masked_fts\n",
    "        \"\"\"\n",
    "        # interpolate on three dim data:\n",
    "        fts = F.interpolate(fts, size=mask.shape[-1:], mode='linear')\n",
    "        # print(mask.shape[-1:])\n",
    "        # fts = fts\n",
    "        # print(fts.shape)\n",
    "        # print('fts shape:', fts.shape)x\n",
    "        masked_fts = torch.sum(fts * mask[None, ...], dim=(2)) \\\n",
    "            / (mask[None, ...].sum(dim=(2)) + 1e-5)\n",
    "        return masked_fts\n",
    "        \n",
    "    def getPrototype(self, fg_fts, bg_fts):\n",
    "        \"\"\"\n",
    "        Average the features to obtain the prototype\n",
    "\n",
    "        Args:\n",
    "            fg_fts: lists of list of foreground features for each way/shot\n",
    "                expect shape: Wa x Sh x [1 x C]\n",
    "            bg_fts: lists of list of background features for each way/shot\n",
    "                expect shape: Wa x Sh x [1 x C]\n",
    "        \"\"\"\n",
    "        n_ways, n_shots = len(fg_fts), len(fg_fts[0])\n",
    "        fg_prototypes = [sum(way) / n_shots for way in fg_fts]\n",
    "        bg_prototype = sum([sum(way) / n_shots for way in bg_fts]) / n_ways\n",
    "        return fg_prototypes, bg_prototype\n",
    "    \n",
    "    def alignLoss(self, qry_fts, pred, supp_fts, fore_mask, back_mask):\n",
    "        \"\"\"\n",
    "        Compute the loss for the prototype alignment branch\n",
    "\n",
    "        Args:\n",
    "            qry_fts: embedding features for query images\n",
    "                expect shape: N x C x H'\n",
    "            pred: predicted segmentation score\n",
    "                expect shape: N x (1 + Wa) x H\n",
    "            supp_fts: embedding features for support images\n",
    "                expect shape: Wa x Sh x C x H'\n",
    "            fore_mask: foreground masks for support images\n",
    "                expect shape: way x shot x H\n",
    "            back_mask: background masks for support images\n",
    "                expect shape: way x shot x H\n",
    "        \"\"\"\n",
    "        n_ways, n_shots = len(fore_mask), len(fore_mask[0])\n",
    "\n",
    "        # Mask and get query prototype\n",
    "        pred_mask = pred.argmax(dim=1, keepdim=True)\n",
    "        binary_masks = [pred_mask == i for i in range(1 + n_ways)]\n",
    "        skip_ways = [i for i in range(n_ways) if binary_masks[i + 1].sum() == 0]\n",
    "        pred_mask = torch.stack(binary_masks, dim=1).float()\n",
    "        # print(pred_mask.shape, qry_fts.shape, (qry_fts.unsqueeze(1) * pred_mask).shape)\n",
    "        qry_prototypes = torch.sum(qry_fts.unsqueeze(1) * pred_mask, dim=(0, 3))\n",
    "        qry_prototypes = qry_prototypes / (pred_mask.sum((0, 3)) + 1e-5)\n",
    "        # print('qry_prototypes shape:', qry_prototypes.shape)\n",
    "        # Compute the support loss\n",
    "        loss = 0\n",
    "        for way in range(n_ways):\n",
    "            if way in skip_ways:\n",
    "                continue\n",
    "            # Get the query prototypes\n",
    "            prototypes = [qry_prototypes[[0]], qry_prototypes[[way + 1]]]\n",
    "            for shot in range(n_shots):\n",
    "                img_fts = supp_fts[way, [shot]]\n",
    "                supp_dist = [self.calDist(img_fts, prototype) for prototype in prototypes]\n",
    "                supp_pred = torch.stack(supp_dist, dim=1)\n",
    "                supp_pred = F.interpolate(supp_pred, size=fore_mask.shape[-1],\n",
    "                                          mode='linear')\n",
    "                # Construct the support Ground-Truth segmentation\n",
    "                supp_label = torch.full_like(fore_mask[way, shot], 255,\n",
    "                                             device=img_fts.device).long()\n",
    "                supp_label[fore_mask[way, shot] == 1] = 1\n",
    "                supp_label[back_mask[way, shot] == 1] = 0\n",
    "                # Compute Loss\n",
    "                loss = loss + F.cross_entropy(\n",
    "                    supp_pred, supp_label[None, ...], ignore_index=255) / n_shots / n_ways\n",
    "        return loss\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 100])"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dummy model for testing\n",
    "model = time_FewShotSeg()\n",
    "model.eval()\n",
    "\n",
    "s = time_Encoder(6, None)\n",
    "s(torch.rand(2, 6, 100)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy input\n",
    "supp_imgs = [[torch.rand(1, 6, 100) for _ in range(5)] for _ in range(5)]\n",
    "fore_mask = [[torch.rand(1, 100) for _ in range(5)] for _ in range(5)]\n",
    "back_mask = [[torch.rand(1, 100) for _ in range(5)] for _ in range(5)]\n",
    "qry_imgs = [torch.rand(1, 6, 100) for _ in range(5)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_ways: 5 n_shots: 5 n_queries: 5 batch_size: 1 img_size: torch.Size([100])\n",
      "supp_imgs shape: torch.Size([1, 6, 100])\n",
      "after concat torch.Size([30, 6, 100]) which is (Wa*Sh + N) x B x 6 x 100, Wa = 5 Sh = 5 N = 5 B = 1\n",
      "torch.Size([30, 6, 100])\n",
      "after go through encoder torch.Size([30, 512, 100])\n",
      "fts_size: torch.Size([100])\n",
      "supp_fts shape: torch.Size([5, 5, 1, 512, 100])\n",
      "qry_fts shape: torch.Size([5, 1, 512, 100])\n",
      "fore_mask shape: torch.Size([5, 5, 1, 100])\n",
      "back_mask shape: torch.Size([5, 5, 1, 100])\n",
      "fg_prototypes shape: 5 torch.Size([1, 512])\n",
      "bg_prototype shape: torch.Size([1, 512])\n",
      "prototypes shape: 6 torch.Size([1, 512])\n",
      "dist shape: 6 torch.Size([5, 100])\n",
      "pred shape: torch.Size([5, 6, 100])\n",
      "upsampling torch.Size([5, 6, 100])\n",
      "outputs shape: 1 torch.Size([5, 6, 100])\n",
      "outputs shape: 1 torch.Size([5, 6, 100])\n",
      "torch.Size([5, 6, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 6, 100]), tensor(nan, grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run\n",
    "output, align_loss = model(supp_imgs, fore_mask, back_mask, qry_imgs)\n",
    "\n",
    "output.shape, align_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nn_ways: 5 n_shots: 5 n_queries: 5 batch_size: 2 img_size: torch.Size([125, 125])\\nsupp_imgs shape: torch.Size([2, 3, 125, 125])\\nafter concat torch.Size([60, 3, 125, 125]) which is (Wa*Sh + N) x B x 3 x H x W, Wa = 5 Sh = 5 N = 5 B = 2\\nafter go through encoder torch.Size([60, 512, 4, 4])\\nfts_size: torch.Size([4, 4])\\nsupp_fts shape: torch.Size([5, 5, 2, 512, 4, 4])\\nqry_fts shape: torch.Size([5, 2, 512, 4, 4])\\nfore_mask shape: torch.Size([5, 5, 2, 125, 125])\\nback_mask shape: torch.Size([5, 5, 2, 125, 125])\\nsupp_fg_fts shape: 5 5 torch.Size([1, 512])\\nfg_prototypes shape: 5 torch.Size([1, 512])\\nbg_prototype shape: torch.Size([1, 512])\\nprototypes shape: 6 torch.Size([1, 512])\\ndist shape: 6 torch.Size([5, 4, 4])\\npred shape: torch.Size([5, 6, 4, 4])\\nUPSAMPLING: outputs shape: 1 torch.Size([5, 6, 125, 125])\\n\\n.....\\nn_ways: 5 n_shots: 5 n_queries: 5 batch_size: 2 img_size: torch.Size([100])\\nsupp_imgs shape: torch.Size([2, 6, 100])\\nafter concat torch.Size([60, 6, 100]) which is (Wa*Sh + N) x B x 6 x 100, Wa = 5 Sh = 5 N = 5 B = 2\\nafter go through encoder torch.Size([60, 512, 100])\\nfts_size: torch.Size([100])\\nsupp_fts shape: torch.Size([5, 5, 2, 512, 100])\\nqry_fts shape: torch.Size([5, 2, 512, 100])\\nfore_mask shape: torch.Size([5, 5, 2, 100])\\nback_mask shape: torch.Size([5, 5, 2, 100])\\nsupp_fg_fts shape: 5 5 torch.Size([1, 512])\\nfg_prototypes shape: 5 torch.Size([1, 512])\\nbg_prototype shape: torch.Size([1, 512])\\nprototypes shape: 6 torch.Size([1, 512])\\ndist shape: 6 torch.Size([5, 100])\\npred shape: torch.Size([5, 6, 100])\\noutputs shape: 1 torch.Size([5, 6, 100])\\n'"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "n_ways: 5 n_shots: 5 n_queries: 5 batch_size: 2 img_size: torch.Size([125, 125])\n",
    "supp_imgs shape: torch.Size([2, 3, 125, 125])\n",
    "after concat torch.Size([60, 3, 125, 125]) which is (Wa*Sh + N) x B x 3 x H x W, Wa = 5 Sh = 5 N = 5 B = 2\n",
    "after go through encoder torch.Size([60, 512, 4, 4])\n",
    "fts_size: torch.Size([4, 4])\n",
    "supp_fts shape: torch.Size([5, 5, 2, 512, 4, 4])\n",
    "qry_fts shape: torch.Size([5, 2, 512, 4, 4])\n",
    "fore_mask shape: torch.Size([5, 5, 2, 125, 125])\n",
    "back_mask shape: torch.Size([5, 5, 2, 125, 125])\n",
    "supp_fg_fts shape: 5 5 torch.Size([1, 512])\n",
    "fg_prototypes shape: 5 torch.Size([1, 512])\n",
    "bg_prototype shape: torch.Size([1, 512])\n",
    "prototypes shape: 6 torch.Size([1, 512])\n",
    "dist shape: 6 torch.Size([5, 4, 4])\n",
    "pred shape: torch.Size([5, 6, 4, 4])\n",
    "UPSAMPLING: outputs shape: 1 torch.Size([5, 6, 125, 125])\n",
    "torch.Size([10, 6, 125, 125])\n",
    ".....\n",
    "n_ways: 5 n_shots: 5 n_queries: 5 batch_size: 2 img_size: torch.Size([100])\n",
    "supp_imgs shape: torch.Size([2, 6, 100])\n",
    "after concat torch.Size([60, 6, 100]) which is (Wa*Sh + N) x B x 6 x 100, Wa = 5 Sh = 5 N = 5 B = 2\n",
    "after go through encoder torch.Size([60, 512, 100])\n",
    "fts_size: torch.Size([100])\n",
    "supp_fts shape: torch.Size([5, 5, 2, 512, 100])\n",
    "qry_fts shape: torch.Size([5, 2, 512, 100])\n",
    "fore_mask shape: torch.Size([5, 5, 2, 100])\n",
    "back_mask shape: torch.Size([5, 5, 2, 100])\n",
    "supp_fg_fts shape: 5 5 torch.Size([1, 512])\n",
    "fg_prototypes shape: 5 torch.Size([1, 512])\n",
    "bg_prototype shape: torch.Size([1, 512])\n",
    "prototypes shape: 6 torch.Size([1, 512])\n",
    "dist shape: 6 torch.Size([5, 100])\n",
    "pred shape: torch.Size([5, 6, 100])\n",
    "outputs shape: 1 torch.Size([5, 6, 100])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
