{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8145, 100, 220]) torch.Size([8145, 100])\n"
     ]
    }
   ],
   "source": [
    "from utilities import sliding_windows\n",
    "\n",
    "\n",
    "inp = np.load('./datasets/OpportunityUCIDataset/loco_2_mask.npy', allow_pickle=True)\n",
    "inp.item()['inputs'].shape\n",
    "inputs, labels = inp.item()['inputs'], inp.item()['labels']\n",
    "sw = sliding_windows(100, 50)\n",
    "segmented_samples, segmented_labels = sw(torch.tensor(inputs), torch.tensor(labels))\n",
    "print(segmented_samples.shape, segmented_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom Dataset:\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, label, transform=None):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        if self.transform:\n",
    "            return self.transform(self.data[idx]), self.label[idx]\n",
    "        return self.data[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import seed\n",
    "from utils_loader import MultiEpochsDataLoader\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "import torch\n",
    "\n",
    "class StandardTransform(torch.nn.Module):\n",
    "    def __init__(self, scaler='standard'):\n",
    "        super(StandardTransform, self).__init__()\n",
    "        if scaler == 'standard':\n",
    "            self.scaler = StandardScaler()\n",
    "        else:\n",
    "            raise NotImplementedError('Only standard scaler is implemented')\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        data = self.scaler.transform(data)\n",
    "        return torch.tensor(data)\n",
    "    \n",
    "    def fit(self, data):\n",
    "        n_samples, n_time_steps, n_features = data.shape\n",
    "        data_reshaped = data.reshape(-1, n_features)  # The shape becomes (n_samples * n_time_steps, n_features)\n",
    "        self.scaler.fit(data_reshaped)\n",
    "        print('Fitted with mean: {}, and std: {}'.format(self.scaler.mean_, np.sqrt(self.scaler.var_)))\n",
    "        return self\n",
    "    \n",
    "\n",
    "seed(73054772)\n",
    "# create train, val, test dataset:\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(segmented_samples, segmented_labels, test_size=0.3, random_state=42, shuffle=True)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "d, l = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import ModuleList\n",
    "\n",
    "from torch.nn.modules.transformer import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "\n",
    "class SimpleSegmenterMaskTransformer(nn.Module):\n",
    "    def __init__(self, in_channels=6, num_layers=2, num_heads=4, embed_dims=256, **kwargs):\n",
    "        super(SimpleSegmenterMaskTransformer, self).__init__(**kwargs)\n",
    "\n",
    "        # Fixed parameters for simplicity\n",
    "        mlp_ratio = 4\n",
    "        # norm_cfg = dict(type='LN')\n",
    "        # act_cfg = dict(type='GELU')\n",
    "        self.init_std = 0.02\n",
    "        self.num_classes = 5\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            layer = nn.TransformerEncoderLayer(\n",
    "                d_model=embed_dims,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=mlp_ratio * embed_dims,\n",
    "                dropout=0.5,\n",
    "                activation=F.gelu,\n",
    "                layer_norm_eps=1e-05,\n",
    "                batch_first=True,\n",
    "                norm_first=False,\n",
    "                bias=True\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.dec_proj = nn.Linear(in_channels, embed_dims)\n",
    "        self.cls_emb = nn.Parameter(torch.randn(1, self.num_classes, embed_dims))\n",
    "        self.patch_proj = nn.Linear(embed_dims, embed_dims, bias=False)\n",
    "        self.classes_proj = nn.Linear(embed_dims, embed_dims, bias=False)\n",
    "        self.decoder_norm = nn.LayerNorm(embed_dims)\n",
    "        self.mask_norm = nn.LayerNorm(self.num_classes)\n",
    "        self.init_weights()\n",
    "    def init_weights(self):\n",
    "        nn.init.trunc_normal_(self.cls_emb, std=self.init_std)\n",
    "        nn.init.trunc_normal_(self.patch_proj.weight, std=self.init_std)\n",
    "        nn.init.trunc_normal_(self.classes_proj.weight, std=self.init_std)\n",
    "        # Initialize weights for Transformer layers\n",
    "        for layer in self.layers:\n",
    "            for param in layer.parameters():\n",
    "                if param.dim() > 1:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs.permute(0, 2, 1) # b h c\n",
    "        b, c, h = x.shape\n",
    "        x = x.view(b, c, -1).permute(0, 2, 1)\n",
    "\n",
    "        x = self.dec_proj(x)\n",
    "        cls_emb = self.cls_emb.expand(x.size(0), -1, -1)\n",
    "        x = torch.cat((x, cls_emb), 1)\n",
    "        for layer in self.layers: \n",
    "            x = layer(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        patches = self.patch_proj(x[:, :-self.num_classes]) # shape 128, 75, 64\n",
    "        cls_seg_feat = self.classes_proj(x[:, -self.num_classes:])\n",
    "        # cls_seg_feat = nn.functional.dropout(cls_seg_feat, p=0.5, training=self.training)\n",
    "        patches = F.normalize(patches, dim=2, p=2)\n",
    "        cls_seg_feat = F.normalize(cls_seg_feat, dim=2, p=2)\n",
    "        # print(patches.shape)\n",
    "        masks = patches @ cls_seg_feat.transpose(1, 2)\n",
    "        masks = self.mask_norm(masks).contiguous().view(b, h, -1)\n",
    "\n",
    "        return masks\n",
    "\n",
    "    def forward_pred(self, inputs):\n",
    "        masks = self.forward(inputs)\n",
    "        masks = masks.permute(0, 2, 1)\n",
    "        probabilities = F.softmax(masks, dim=1)\n",
    "        pred = torch.argmax(probabilities, dim=1)\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_iou_time_series(pred, gt, num_classes=7):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Intersection over Union (mIoU) for time series data.\n",
    "    \n",
    "    Parameters:\n",
    "        pred (torch.Tensor): Predictions, assumed to be one-hot encoded.\n",
    "        gt (torch.Tensor): Ground truth labels, assumed to be one-hot encoded.\n",
    "        num_classes (int): Number of classes in the segmentation task.\n",
    "\n",
    "    Returns:\n",
    "        float: Mean IoU for the batch.\n",
    "    \"\"\"\n",
    "    # Initialize variables to store IoU for each class\n",
    "    iou_list = []\n",
    "\n",
    "    # Loop through each class\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = pred == cls\n",
    "        gt_inds = gt == cls\n",
    "\n",
    "        # Calculate Intersection and Union\n",
    "        # print(pred_inds.shape, gt_inds.shape)\n",
    "        intersection = torch.sum(pred_inds & gt_inds, dim=1).float()\n",
    "        union = torch.sum(pred_inds | gt_inds, dim=1).float()\n",
    "\n",
    "        # Calculate IoU. Avoid division by zero by adding a small epsilon.\n",
    "        iou = intersection / (union + 1e-8)\n",
    "\n",
    "        # Append the mean IoU for this class\n",
    "        iou_list.append(torch.mean(iou))\n",
    "\n",
    "    # Calculate the mean IoU across all classes\n",
    "    mean_iou = torch.mean(torch.stack(iou_list))\n",
    "\n",
    "    return mean_iou.item()\n",
    "\n",
    "\n",
    "def mean_iou(preds, labels, num_classes):\n",
    "    # Flatten the predictions and labels, this comes from the argmax of the one-hot vectors\n",
    "    preds = preds.view(-1)\n",
    "    labels = labels.view(-1)\n",
    "\n",
    "    # Create confusion matrix\n",
    "    confusion_matrix = torch.zeros(num_classes, num_classes, dtype=torch.int64)\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            confusion_matrix[i, j] = torch.sum((preds == i) & (labels == j))\n",
    "\n",
    "    # Calculate IoU for each class\n",
    "    ious = []\n",
    "    for i in range(num_classes):\n",
    "        true_positive = confusion_matrix[i, i]\n",
    "        false_positive = confusion_matrix[i, :].sum() - true_positive\n",
    "        false_negative = confusion_matrix[:, i].sum() - true_positive\n",
    "\n",
    "        # Avoid division by zero\n",
    "        union = true_positive + false_positive + false_negative\n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))  # No predictions and no labels for this class\n",
    "        else:\n",
    "            ious.append(true_positive.float() / union.float())\n",
    "\n",
    "    # Calculate mean IoU\n",
    "    ious = torch.tensor(ious)\n",
    "    mean_iou = torch.nanmean(ious)  # Mean over all classes, ignoring NaNs\n",
    "    return mean_iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available.\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the CNN model\n",
    "from livelossplot import PlotLosses\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from utilities import get_device\n",
    "num_epochs = 100\n",
    "device = get_device()\n",
    "def c_loss(logits):\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "    one_hot_predictions = torch.argmax(probabilities, dim=1)\n",
    "    product_prob = torch.prod(probabilities, dim=1)\n",
    "    changes = torch.abs(one_hot_predictions[:, 1:]*product_prob[:, 1:] - one_hot_predictions[:, :-1]*product_prob[:, :-1])\n",
    "    penalty = changes.sum(dim=1).float().mean()\n",
    "    return penalty + 1e-4\n",
    "def custom_loss(logits, targets, penalty_weight=0.1):\n",
    "    \"\"\"\n",
    "    Custom loss function that penalizes frequent changes in the prediction sequence.\n",
    "    \n",
    "    Args:\n",
    "    - logits: Tensor of shape [N, C, L], where N is batch size, C is number of classes, and L is sequence length.\n",
    "    - targets: Tensor of shape [N, L] containing class indices for each element in the sequence.\n",
    "    - penalty_weight: Weight of the penalty term.\n",
    "\n",
    "    Returns:\n",
    "    - Total loss with the penalty.\n",
    "    \"\"\"\n",
    "\n",
    "    # Standard cross-entropy loss\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "    ce_loss = F.cross_entropy(probabilities, targets)\n",
    "\n",
    "    penalty = c_loss(logits)\n",
    "    total_loss = ce_loss #penalty * penalty_weight\n",
    "\n",
    "    return total_loss, ce_loss.item(), penalty.item()\n",
    "\n",
    "def plotASample(data, label):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)  # Add a new subplot to the figure\n",
    "    ax.plot(data, label='Data')  # Plot the data\n",
    "    ax.plot(label, label='Label')  # Plot the label\n",
    "    ax.legend()  # Add a legend\n",
    "    fig.canvas.draw()  # Redraw the figure\n",
    "    plt.plot()\n",
    "\n",
    "\n",
    "model = SimpleSegmenterMaskTransformer(in_channels=220).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion =custom_loss #nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.00)\n",
    "liveloss = PlotLosses()\n",
    "\n",
    "# Iterate over the training data\n",
    "logs = {}\n",
    "# change the plt size:\n",
    "best_loss = math.inf\n",
    "counter_i = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in train_loader:\n",
    "        # Forward pass\n",
    "        images = images.float().to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        outputs = outputs.permute(0, 2, 1)\n",
    "        loss, tra_ce, tra_pen = criterion(outputs, labels.long())\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # train loss and accuracy check:\n",
    "        if counter_i % 1000 == 0 and counter_i != 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # total = labels.size(0)\n",
    "            # correct = (predicted == labels).sum().item()\n",
    "            # print('Accuracy: {:.2f}%'.format(correct / total * 100))\n",
    "            logs['loss'] = loss.item()\n",
    "            logs['ce'] = tra_ce\n",
    "            logs['pen'] = tra_pen\n",
    "            # logs['accuracy'] = correct / total * 100\n",
    "    \n",
    "            # validation loss and accuracy check:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in val_loader:\n",
    "                images = images.float().to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                outputs = outputs.permute(0, 2, 1) #view(images.shape[0], 2, -1)\n",
    "                loss, val_ce, val_pen = criterion(outputs, labels.long())\n",
    "            logs['val_loss'] = loss.item()\n",
    "            logs['val_ce'] = val_ce\n",
    "            logs['val_pen'] = val_pen\n",
    "            logs['miou'] = mean_iou(model.forward_pred(images), labels.long(),num_classes=5)\n",
    "            if best_loss > loss.item():\n",
    "                best_loss = loss.item()\n",
    "                torch.save(model.state_dict(), './saved_model/best_transformer_oppo.pth')\n",
    "            pred = torch.argmax(torch.softmax(outputs[0], dim=0), dim=0).detach().cpu().numpy()\n",
    "            plotASample(images[0].detach().cpu().numpy()[1:7], pred)\n",
    "            liveloss.update(logs)\n",
    "            liveloss.send()\n",
    "            # logs['val_accuracy'] = correct / total * 100\n",
    "        counter_i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
