{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from methods import Segmenter, TransformerModel, LSTM, CRNN, UNet,CCRNN, UNet2, PatchTST\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from methods.unetc import UNetc\n",
    "from methods.unetr import UNetrt\n",
    "from utilities import printc, seed\n",
    "from utils_loader import get_dataloaders, test_idea_dataloader_ABC_to_BCA, test_idea_dataloader_long_A_B_to_AB\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from utils_metrics import eval_dense_label_to_classification, mean_iou, visualize_softmax\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "from utilities import sliding_windows\n",
    "import torch\n",
    "\n",
    "def load_data():\n",
    "    filename = './datasets/spar/spar_dataset'\n",
    "    # read through the folder that end with csv:\n",
    "    sw = sliding_windows(300, 150)\n",
    "    def process(files, sw, le: LabelEncoder):\n",
    "        nps = []\n",
    "        labels = []\n",
    "        for file in files:\n",
    "            df = pd.read_csv(os.path.join(filename, file))\n",
    "            nps.append(df.to_numpy()[:, 1:7])\n",
    "            labels.append([le.transform([file.split('_')[1]])[0]]*df.shape[0])\n",
    "        nps = np.concatenate(nps, axis=0)\n",
    "        labels = np.concatenate(labels, axis=0)\n",
    "        return sw(torch.tensor(nps).float(), torch.tensor(labels))\n",
    "    \n",
    "    classes = []\n",
    "    subjs = []\n",
    "    for root, dirs, files in os.walk(filename):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\") and 'R' in file:\n",
    "                label = file.split('_')[1]\n",
    "                subj = file.split('_')[0]\n",
    "                if label not in classes:\n",
    "                    classes.append(label)\n",
    "                if subj not in subjs:\n",
    "                    subjs.append(subj)\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le.fit(classes)\n",
    "    \n",
    "    # read through the folder that end with csv:\n",
    "    samples = []\n",
    "    labels = []\n",
    "    for each_subj in subjs:\n",
    "        collection = {class_name: [] for class_name in classes}\n",
    "        for root, dirs, files in os.walk(filename):\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\") and 'R' in file and each_subj == file.split('_')[0]:\n",
    "                    label = file.split('_')[1]\n",
    "                    collection[label].append(file)\n",
    "        leftover = []\n",
    "        for key in collection:\n",
    "            # sort the files\n",
    "            sorted_files = sorted(collection[key], key=lambda x: int(x.split('_')[3].split('.')[0]))\n",
    "            sample, lab = process(sorted_files, sw, le)\n",
    "            samples.append(sample)\n",
    "            labels.append(lab)\n",
    "    samples = torch.cat(samples, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    return samples, labels\n",
    "    \n",
    "\n",
    "def load_data1():\n",
    "    filename = './datasets/spar/spar_dataset'\n",
    "    seed = 0\n",
    "    # read through the folder that end with csv:\n",
    "    def combo_to_np(combo, le: LabelEncoder):\n",
    "        samples = []\n",
    "        labels = []\n",
    "        for each_combo in combo:\n",
    "            for each_file in each_combo:\n",
    "                df = pd.read_csv(os.path.join(filename, each_file))\n",
    "                samples.append(df.to_numpy()[:, 1:7])\n",
    "                transformed = le.transform([each_file.split('_')[1]])[0] + 1\n",
    "                labels.append([transformed] * df.shape[0])\n",
    "                # print('labels', labels)\n",
    "        # convert to np:\n",
    "        samples = np.concatenate(samples)\n",
    "        labels = np.concatenate(labels)\n",
    "        # print(samples.shape, labels.shape)\n",
    "        return samples, labels\n",
    "    \n",
    "    if True:\n",
    "        filename = './datasets/spar/spar_dataset'\n",
    "        seed = 0\n",
    "        # read through the folder that end with csv:\n",
    "        classes = []\n",
    "        subjs = []\n",
    "        for root, dirs, files in os.walk(filename):\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\") and 'R' in file:\n",
    "                    label = file.split('_')[1]\n",
    "                    subj = file.split('_')[0]\n",
    "                    if label not in classes:\n",
    "                        classes.append(label)\n",
    "                    if subj not in subjs:\n",
    "                        subjs.append(subj)\n",
    "                    # print(os.path.join(root, file))\n",
    "                    # df = pd.read_csv(os.path.join(root, file))\n",
    "\n",
    "        # generate train combo => ABC, ABD, ... # test combo => BCD (some combo that was not seem in train) based on the classes\n",
    "        train_combo = [('E1', 'E2', 'E3')]\n",
    "        test_combo = [('E3', 'E2', 'E1')]\n",
    "\n",
    "        assert all([each in classes for comb in train_combo for each in comb]), 'train combo not in classes'\n",
    "        assert all([each in classes for comb in test_combo for each in comb]), 'test combo not in classes'\n",
    "        train_classes = list(set([each for comb in train_combo for each in comb]))\n",
    "        le = LabelEncoder()\n",
    "        le.fit(train_classes)\n",
    "        random.seed(seed)\n",
    "        train_samples = []\n",
    "        test_samples = []\n",
    "        for each_subj in subjs:\n",
    "            collection = {class_name: [] for class_name in classes}\n",
    "            for root, dirs, files in os.walk(filename):\n",
    "                for file in files:\n",
    "                    if file.endswith(\".csv\") and 'R' in file and each_subj == file.split('_')[0]:\n",
    "                        label = file.split('_')[1]\n",
    "                        collection[label].append(file)\n",
    "\n",
    "            lengths = [len(collection[class_name]) for class_name in classes]\n",
    "            minimum = min(lengths)\n",
    "            # 80% for train, 20% for test\n",
    "            train_length = int(minimum * 0.8)\n",
    "            test_length = minimum - train_length\n",
    "            # print(f'{each_subj} has {minimum} samples, {train_length} for train, {test_length} for test')\n",
    "            # generate train and test data\n",
    "            \n",
    "            # randomly extract one sample of label from the collection and remove that label from the list in the collection:\n",
    "            for _ in range(train_length):\n",
    "                for i in train_combo:\n",
    "                    cur_combo = []\n",
    "                    for j in i:\n",
    "                        selected_sample = random.choice(collection[j])\n",
    "                        collection[j].remove(selected_sample)\n",
    "                        cur_combo.append(selected_sample)\n",
    "                    train_samples.append(cur_combo)\n",
    "            for _ in range(test_length):\n",
    "                for i in test_combo:\n",
    "                    cur_combo = []\n",
    "                    for j in i:\n",
    "                        selected_sample = random.choice(collection[j])\n",
    "                        collection[j].remove(selected_sample)\n",
    "                        cur_combo.append(selected_sample)\n",
    "                    test_samples.append(cur_combo)\n",
    "        train_s = combo_to_np(train_samples, le)\n",
    "        test_s = combo_to_np(test_samples, le)\n",
    "    \n",
    "    sw = sliding_windows(300, 150)\n",
    "    \n",
    "    train_samples, train_labels = sw(torch.tensor(train_s[0]), torch.tensor(train_s[1]))\n",
    "    test_samples, test_labels = sw(torch.tensor(test_s[0]), torch.tensor(test_s[1]))\n",
    "    # Split the dataset into train, val and test:\n",
    "    train_samples, val_samples, train_labels, val_labels = train_test_split(train_samples, train_labels, test_size=0.2, random_state=42)\n",
    "    return train_samples, val_samples, test_samples, train_labels, val_labels, test_labels\n",
    "\n",
    "# samples, labels = load_data() \n",
    "train_samples, val_samples, test_samples, train_labels, val_labels, test_labels = load_data1()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset for ctc:\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, samples, labels):\n",
    "        self.samples = samples\n",
    "        self.labels = self._get_labels_ctc(labels)\n",
    "        self.sec_labels = labels\n",
    "    def _get_labels_ctc(self, labels):\n",
    "        # convert the labels to ctc format\n",
    "        # [1, 1, 1, 2, 2, 2, 3, 3, 3] -> [1, 2, 3], 3\n",
    "        lis = []\n",
    "        \n",
    "        # when the new label is different from the previous one, append the previous one to the list\n",
    "        for i in range(len(labels)):\n",
    "            cur_list = []\n",
    "            prev = labels[i, 0]\n",
    "            count = 1\n",
    "            for j in range(1, len(labels[i])):\n",
    "                if labels[i, j] == prev:\n",
    "                    if j == len(labels[i]) - 1:\n",
    "                        cur_list.append(prev)\n",
    "                else:\n",
    "                    cur_list.append(prev)\n",
    "                    prev = labels[i, j]\n",
    "                    count += 1\n",
    "            cur_list = torch.tensor(cur_list)\n",
    "            lis.append(cur_list)\n",
    "        return lis\n",
    "    @staticmethod\n",
    "    def custom_collate_fn(x):\n",
    "        batch = []\n",
    "        labels = []\n",
    "        lengths = []\n",
    "        for i in range(len(x)):\n",
    "            batch.append(x[i][0])\n",
    "            labels.extend(x[i][1])\n",
    "            lengths.append(len(x[i][1]))\n",
    "            # lengths.append()\n",
    "        batch = torch.stack(batch)\n",
    "        labels = torch.tensor(labels)\n",
    "        lengths = torch.tensor(lengths)\n",
    "        return batch, labels, lengths\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        # target, target_length\n",
    "        return self.samples[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 0.0349,  1.0595,  0.2088,  0.6428,  1.9648, -0.1645],\n",
      "        [ 0.0464,  0.9939,  0.1085,  0.6062,  1.9830, -0.1459],\n",
      "        [ 0.0426,  0.9146,  0.0546, -0.1829,  1.9559, -0.1445],\n",
      "        ...,\n",
      "        [ 0.2131, -0.0094, -0.5881, -0.4107,  3.2487, -0.3954],\n",
      "        [ 0.2324, -0.0292, -0.4669, -0.1402,  3.2083, -0.3127],\n",
      "        [ 0.3251, -0.0484, -0.4190,  0.1312,  3.1519, -0.2248]],\n",
      "       dtype=torch.float64), tensor([3, 1, 2]))\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(train_samples, train_labels)\n",
    "val_dataset = CustomDataset(val_samples, val_labels)\n",
    "test_dataset = CustomDataset(test_samples, test_labels)\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    print(train_dataset[i])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=CustomDataset.custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=CustomDataset.custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=CustomDataset.custom_collate_fn)\n",
    "\n",
    "# z = next(iter(train_loader))\n",
    "z = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm:\n",
    "\n",
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, num_classes)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9s/zgph6l3n6b75g5pycfr4d_n40000gn/T/ipykernel_44330/1389195309.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_count = torch.tensor(labels_count).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [10/19], Loss: 91.8151\n",
      "Epoch [2/50], Step [10/19], Loss: 4.9258\n",
      "Epoch [3/50], Step [10/19], Loss: 22.2189\n",
      "Epoch [4/50], Step [10/19], Loss: 21.5058\n",
      "Epoch [5/50], Step [10/19], Loss: 6.5217\n",
      "Epoch [6/50], Step [10/19], Loss: 8.8896\n",
      "Epoch [7/50], Step [10/19], Loss: 21.4771\n",
      "Epoch [8/50], Step [10/19], Loss: 7.1393\n",
      "Epoch [9/50], Step [10/19], Loss: 21.2490\n",
      "Epoch [10/50], Step [10/19], Loss: 21.0558\n",
      "Epoch [11/50], Step [10/19], Loss: 5.3303\n",
      "Epoch [12/50], Step [10/19], Loss: 33.6539\n",
      "Epoch [13/50], Step [10/19], Loss: 37.1955\n",
      "Epoch [14/50], Step [10/19], Loss: 9.3190\n",
      "Epoch [15/50], Step [10/19], Loss: 21.4430\n",
      "Epoch [16/50], Step [10/19], Loss: 34.3648\n",
      "Epoch [17/50], Step [10/19], Loss: 6.1906\n",
      "Epoch [18/50], Step [10/19], Loss: 10.1343\n",
      "Epoch [19/50], Step [10/19], Loss: 20.9200\n",
      "Epoch [20/50], Step [10/19], Loss: 35.6347\n",
      "Epoch [21/50], Step [10/19], Loss: 7.2981\n",
      "Epoch [22/50], Step [10/19], Loss: 21.9519\n",
      "Epoch [23/50], Step [10/19], Loss: 36.6023\n",
      "Epoch [24/50], Step [10/19], Loss: 19.9972\n",
      "Epoch [25/50], Step [10/19], Loss: 21.1700\n",
      "Epoch [26/50], Step [10/19], Loss: 20.5535\n",
      "Epoch [27/50], Step [10/19], Loss: 7.3915\n",
      "Epoch [28/50], Step [10/19], Loss: 8.2913\n",
      "Epoch [29/50], Step [10/19], Loss: 44.7682\n",
      "Epoch [30/50], Step [10/19], Loss: 21.2188\n",
      "Epoch [31/50], Step [10/19], Loss: 20.8574\n",
      "Epoch [32/50], Step [10/19], Loss: 21.1838\n",
      "Epoch [33/50], Step [10/19], Loss: 6.8241\n",
      "Epoch [34/50], Step [10/19], Loss: 6.9525\n",
      "Epoch [35/50], Step [10/19], Loss: 8.6333\n",
      "Epoch [36/50], Step [10/19], Loss: 20.0398\n",
      "Epoch [37/50], Step [10/19], Loss: 19.4416\n",
      "Epoch [38/50], Step [10/19], Loss: 37.9107\n",
      "Epoch [39/50], Step [10/19], Loss: 8.1956\n",
      "Epoch [40/50], Step [10/19], Loss: 20.3325\n",
      "Epoch [41/50], Step [10/19], Loss: 8.5755\n",
      "Epoch [42/50], Step [10/19], Loss: 8.0835\n",
      "Epoch [43/50], Step [10/19], Loss: 48.4176\n",
      "Epoch [44/50], Step [10/19], Loss: 54.0778\n",
      "Epoch [45/50], Step [10/19], Loss: 19.8200\n",
      "Epoch [46/50], Step [10/19], Loss: 5.9990\n",
      "Epoch [47/50], Step [10/19], Loss: 7.7902\n",
      "Epoch [48/50], Step [10/19], Loss: 18.3434\n",
      "Epoch [49/50], Step [10/19], Loss: 8.3280\n",
      "Epoch [50/50], Step [10/19], Loss: 16.5799\n",
      "Epoch [51/50], Step [10/19], Loss: 5.4383\n",
      "Epoch [52/50], Step [10/19], Loss: 11.7956\n",
      "Epoch [53/50], Step [10/19], Loss: 11.1157\n",
      "Epoch [54/50], Step [10/19], Loss: 5.6893\n",
      "Epoch [55/50], Step [10/19], Loss: 8.4421\n",
      "Epoch [56/50], Step [10/19], Loss: 6.2361\n",
      "Epoch [57/50], Step [10/19], Loss: 5.0007\n",
      "Epoch [58/50], Step [10/19], Loss: 36.9842\n",
      "Epoch [59/50], Step [10/19], Loss: 6.9444\n",
      "Epoch [60/50], Step [10/19], Loss: 8.1422\n",
      "Epoch [61/50], Step [10/19], Loss: 7.8037\n",
      "Epoch [62/50], Step [10/19], Loss: 4.7200\n",
      "Epoch [63/50], Step [10/19], Loss: 4.8030\n",
      "Epoch [64/50], Step [10/19], Loss: 23.7172\n",
      "Epoch [65/50], Step [10/19], Loss: 4.8501\n",
      "Epoch [66/50], Step [10/19], Loss: 4.0658\n",
      "Epoch [67/50], Step [10/19], Loss: 4.7171\n",
      "Epoch [68/50], Step [10/19], Loss: 20.2989\n",
      "Epoch [69/50], Step [10/19], Loss: 3.5437\n",
      "Epoch [70/50], Step [10/19], Loss: 7.7762\n",
      "Epoch [71/50], Step [10/19], Loss: 4.2383\n",
      "Epoch [72/50], Step [10/19], Loss: 4.1266\n",
      "Epoch [73/50], Step [10/19], Loss: 3.4915\n",
      "Epoch [74/50], Step [10/19], Loss: 3.7207\n",
      "Epoch [75/50], Step [10/19], Loss: 3.7127\n",
      "Epoch [76/50], Step [10/19], Loss: 3.0367\n",
      "Epoch [77/50], Step [10/19], Loss: 2.9673\n",
      "Epoch [78/50], Step [10/19], Loss: 3.6001\n",
      "Epoch [79/50], Step [10/19], Loss: 27.3857\n",
      "Epoch [80/50], Step [10/19], Loss: 2.9074\n",
      "Epoch [81/50], Step [10/19], Loss: 3.3234\n",
      "Epoch [82/50], Step [10/19], Loss: 3.1561\n",
      "Epoch [83/50], Step [10/19], Loss: 2.8148\n",
      "Epoch [84/50], Step [10/19], Loss: 3.1075\n",
      "Epoch [85/50], Step [10/19], Loss: 3.3204\n",
      "Epoch [86/50], Step [10/19], Loss: 2.9071\n",
      "Epoch [87/50], Step [10/19], Loss: 3.2531\n",
      "Epoch [88/50], Step [10/19], Loss: 2.9543\n",
      "Epoch [89/50], Step [10/19], Loss: 23.6265\n",
      "Epoch [90/50], Step [10/19], Loss: 2.6944\n",
      "Epoch [91/50], Step [10/19], Loss: 2.6342\n",
      "Epoch [92/50], Step [10/19], Loss: 3.2215\n",
      "Epoch [93/50], Step [10/19], Loss: 3.4407\n",
      "Epoch [94/50], Step [10/19], Loss: 2.7855\n",
      "Epoch [95/50], Step [10/19], Loss: 2.7412\n",
      "Epoch [96/50], Step [10/19], Loss: 2.4816\n",
      "Epoch [97/50], Step [10/19], Loss: 3.1627\n",
      "Epoch [98/50], Step [10/19], Loss: 2.9259\n",
      "Epoch [99/50], Step [10/19], Loss: 3.2770\n",
      "Epoch [100/50], Step [10/19], Loss: 4.5749\n",
      "Epoch [101/50], Step [10/19], Loss: 2.7201\n",
      "Epoch [102/50], Step [10/19], Loss: 2.6738\n",
      "Epoch [103/50], Step [10/19], Loss: 2.7658\n",
      "Epoch [104/50], Step [10/19], Loss: 2.9509\n",
      "Epoch [105/50], Step [10/19], Loss: 2.8905\n",
      "Epoch [106/50], Step [10/19], Loss: 2.7041\n",
      "Epoch [107/50], Step [10/19], Loss: 2.8921\n",
      "Epoch [108/50], Step [10/19], Loss: 2.6519\n",
      "Epoch [109/50], Step [10/19], Loss: 2.8032\n",
      "Epoch [110/50], Step [10/19], Loss: 29.6783\n",
      "Epoch [111/50], Step [10/19], Loss: 3.0225\n",
      "Epoch [112/50], Step [10/19], Loss: 2.4958\n",
      "Epoch [113/50], Step [10/19], Loss: 2.2195\n",
      "Epoch [114/50], Step [10/19], Loss: 2.8497\n",
      "Epoch [115/50], Step [10/19], Loss: 2.3702\n",
      "Epoch [116/50], Step [10/19], Loss: 2.7527\n",
      "Epoch [117/50], Step [10/19], Loss: 2.6429\n",
      "Epoch [118/50], Step [10/19], Loss: 3.2129\n",
      "Epoch [119/50], Step [10/19], Loss: 2.6665\n",
      "Epoch [120/50], Step [10/19], Loss: 26.7332\n",
      "Epoch [121/50], Step [10/19], Loss: 2.6796\n",
      "Epoch [122/50], Step [10/19], Loss: 2.5973\n",
      "Epoch [123/50], Step [10/19], Loss: 2.8807\n",
      "Epoch [124/50], Step [10/19], Loss: 2.7116\n",
      "Epoch [125/50], Step [10/19], Loss: 2.6821\n",
      "Epoch [126/50], Step [10/19], Loss: 2.8206\n",
      "Epoch [127/50], Step [10/19], Loss: 2.8056\n",
      "Epoch [128/50], Step [10/19], Loss: 2.6513\n",
      "Epoch [129/50], Step [10/19], Loss: 2.6357\n",
      "Epoch [130/50], Step [10/19], Loss: 2.3520\n",
      "Epoch [131/50], Step [10/19], Loss: 2.2813\n",
      "Epoch [132/50], Step [10/19], Loss: 2.7736\n",
      "Epoch [133/50], Step [10/19], Loss: 2.8474\n",
      "Epoch [134/50], Step [10/19], Loss: 2.3121\n",
      "Epoch [135/50], Step [10/19], Loss: 2.6251\n",
      "Epoch [136/50], Step [10/19], Loss: 2.4236\n",
      "Epoch [137/50], Step [10/19], Loss: 2.5092\n",
      "Epoch [138/50], Step [10/19], Loss: 2.5317\n",
      "Epoch [139/50], Step [10/19], Loss: 2.4964\n",
      "Epoch [140/50], Step [10/19], Loss: 2.7394\n",
      "Epoch [141/50], Step [10/19], Loss: 2.8540\n",
      "Epoch [142/50], Step [10/19], Loss: 2.6066\n",
      "Epoch [143/50], Step [10/19], Loss: 2.7137\n",
      "Epoch [144/50], Step [10/19], Loss: 2.2930\n",
      "Epoch [145/50], Step [10/19], Loss: 2.5550\n",
      "Epoch [146/50], Step [10/19], Loss: 2.6558\n",
      "Epoch [147/50], Step [10/19], Loss: 2.4256\n",
      "Epoch [148/50], Step [10/19], Loss: 2.2075\n",
      "Epoch [149/50], Step [10/19], Loss: 2.5133\n",
      "Epoch [150/50], Step [10/19], Loss: 2.3854\n",
      "Epoch [151/50], Step [10/19], Loss: 2.5287\n",
      "Epoch [152/50], Step [10/19], Loss: 2.1747\n",
      "Epoch [153/50], Step [10/19], Loss: 1.9651\n",
      "Epoch [154/50], Step [10/19], Loss: 2.7539\n",
      "Epoch [155/50], Step [10/19], Loss: 2.3432\n",
      "Epoch [156/50], Step [10/19], Loss: 2.7495\n",
      "Epoch [157/50], Step [10/19], Loss: 2.5433\n",
      "Epoch [158/50], Step [10/19], Loss: 2.5700\n",
      "Epoch [159/50], Step [10/19], Loss: 2.2025\n",
      "Epoch [160/50], Step [10/19], Loss: 2.3700\n",
      "Epoch [161/50], Step [10/19], Loss: 2.5765\n",
      "Epoch [162/50], Step [10/19], Loss: 2.6735\n",
      "Epoch [163/50], Step [10/19], Loss: 2.6729\n",
      "Epoch [164/50], Step [10/19], Loss: 2.5422\n",
      "Epoch [165/50], Step [10/19], Loss: 2.8504\n",
      "Epoch [166/50], Step [10/19], Loss: 2.2886\n",
      "Epoch [167/50], Step [10/19], Loss: 3.0582\n",
      "Epoch [168/50], Step [10/19], Loss: 2.6238\n",
      "Epoch [169/50], Step [10/19], Loss: 2.6375\n",
      "Epoch [170/50], Step [10/19], Loss: 2.6210\n",
      "Epoch [171/50], Step [10/19], Loss: 2.9083\n",
      "Epoch [172/50], Step [10/19], Loss: 2.5184\n",
      "Epoch [173/50], Step [10/19], Loss: 2.2511\n",
      "Epoch [174/50], Step [10/19], Loss: 2.8334\n",
      "Epoch [175/50], Step [10/19], Loss: 2.3601\n",
      "Epoch [176/50], Step [10/19], Loss: 2.2804\n",
      "Epoch [177/50], Step [10/19], Loss: 2.4689\n",
      "Epoch [178/50], Step [10/19], Loss: 2.5592\n",
      "Epoch [179/50], Step [10/19], Loss: 2.6479\n",
      "Epoch [180/50], Step [10/19], Loss: 2.6653\n",
      "Epoch [181/50], Step [10/19], Loss: 2.7799\n",
      "Epoch [182/50], Step [10/19], Loss: 2.1251\n",
      "Epoch [183/50], Step [10/19], Loss: 2.6594\n",
      "Epoch [184/50], Step [10/19], Loss: 2.3459\n",
      "Epoch [185/50], Step [10/19], Loss: 2.5662\n",
      "Epoch [186/50], Step [10/19], Loss: 2.2395\n",
      "Epoch [187/50], Step [10/19], Loss: 1.8396\n",
      "Epoch [188/50], Step [10/19], Loss: 2.4012\n",
      "Epoch [189/50], Step [10/19], Loss: 28.1205\n",
      "Epoch [190/50], Step [10/19], Loss: 2.7668\n",
      "Epoch [191/50], Step [10/19], Loss: 2.5521\n",
      "Epoch [192/50], Step [10/19], Loss: 25.9427\n",
      "Epoch [193/50], Step [10/19], Loss: 2.7714\n",
      "Epoch [194/50], Step [10/19], Loss: 2.6396\n",
      "Epoch [195/50], Step [10/19], Loss: 3.4686\n",
      "Epoch [196/50], Step [10/19], Loss: 2.6685\n",
      "Epoch [197/50], Step [10/19], Loss: 2.4994\n",
      "Epoch [198/50], Step [10/19], Loss: 2.9739\n",
      "Epoch [199/50], Step [10/19], Loss: 1.9915\n",
      "Epoch [200/50], Step [10/19], Loss: 2.4826\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTM(6, 128, 1, 4).to(device)\n",
    "criterion = torch.nn.CTCLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(200):\n",
    "    for i, (images, labels, labels_count) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels_count = torch.tensor(labels_count).to(device)\n",
    "        # Forward pass\n",
    "        images = images.float()\n",
    "        outputs = model(images)\n",
    "        outputs = F.log_softmax(outputs, dim=2)\n",
    "        # print(outputs.shape, labels.shape, labels_count.shape)\n",
    "        input_lengths = torch.full((images.shape[0],), images.shape[1], dtype=torch.long).to(device)\n",
    "        loss = criterion(outputs.transpose(0, 1), labels.cpu(),  input_lengths.cpu(), labels_count.cpu())\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{50}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 3, 4, 4, 4, 4, 3, 3, 3, 2, 2, 3, 3, 3, 3,\n",
      "        2, 2, 3, 3, 3, 2, 2, 3])\n",
      "tensor([2, 1, 3, 2])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "[3, 1, 3, 1, 3, 1, 2, 1, 3, 1, 2, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 0, 0, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 0, 2, 3, 1, 3, 1, 3, 1, 0, 0, 2, 3, 1]\n",
      "[3, 2, 1, 3, 2, 1, 3, 2, 3, 2, 1, 3, 2, 1, 3, 2, 3, 2, 1, 3, 2, 1, 3, 2, 3, 2, 1, 3, 2, 1, 3, 2, 3, 2, 1, 2, 1, 3, 2, 3, 2, 1, 2, 1, 3, 2, 1, 3, 2, 1, 2, 1, 3, 2, 1, 3, 2, 1, 2, 1, 3, 1, 3, 2, 3, 2, 1, 2, 1, 1, 3, 1, 3, 2, 3, 2, 1, 2, 1, 3, 1, 3, 2, 3, 2, 2, 1, 2, 1, 3, 1, 3, 2, 3, 2, 1, 2, 1, 1, 3, 1, 3, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9s/zgph6l3n6b75g5pycfr4d_n40000gn/T/ipykernel_44330/2949538086.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_count = torch.tensor(labels_count).to(device)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/SrBM8AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA4tUlEQVR4nO3deXRUdZ7//1dVkCJqEoyQTQIGQRhlsxHTcUFsIiFfDkO6Z2hhnAEc1J9O6CPGpY2juLTTUVxxhoG2XaJjI0orcBoRpKMJQxuwieQo2nLACYYlhTZKikQJmLq/P+TeSjUBUlTurfLW83FOHa2qe4tPXSrUK5/3Z/EYhmEIAAAgjnlj3QAAAICTIbAAAIC4R2ABAABxj8ACAADiHoEFAADEPQILAACIewQWAAAQ9wgsAAAg7vWIdQO6QzAY1N69e5WSkiKPxxPr5gAAgC4wDEMHDx5UTk6OvN4T96G4IrDs3btXubm5sW4GAAA4Bbt27VK/fv1OeIwrAktKSoqk799wampqjFsDAAC6IhAIKDc31/oePxFXBBazDJSamkpgAQDgB6YrwzkYdAsAAOIegQUAAMQ9AgsAAIh7BBYAABD3CCwAACDuEVgAAEDcI7AAAIC4R2ABAABxj8ACAADiXkSBZdGiRRoxYoS1omxBQYHeeuutE56zbNkyDR06VL169dLw4cO1evXqsOcNw9C8efOUnZ2t5ORkFRYWavv27ZG/EwAA4FoRBZZ+/frp4YcfVl1dnTZv3qyf/OQnmjJlij7++ONOj3/vvfc0ffp0zZ49W1u2bFFJSYlKSkq0detW65j58+fr6aef1uLFi7Vp0yadccYZKioq0qFDh6J7ZwAAwDU8hmEY0bxAenq6Hn30Uc2ePfuY56655hq1trZq1apV1mM//vGPNWrUKC1evFiGYSgnJ0e33Xabbr/9dklSc3OzMjMzVVlZqWnTpnWpDYFAQGlpaWpubmYvIQAAfiAi+f4+5c0P29vbtWzZMrW2tqqgoKDTY2pra1VWVhb2WFFRkVasWCFJamhokN/vV2FhofV8Wlqa8vPzVVtbe9zA0tbWpra2Nut+IBA41bcBoBts2P5XVX26L9bNQIx5PR5NHpmjUbm9Y90UuFDEgeWjjz5SQUGBDh06pDPPPFPLly/XBRdc0Omxfr9fmZmZYY9lZmbK7/dbz5uPHe+YzlRUVOiBBx6ItOkAbHLra/X68mDbyQ+E623e+ZVWzrk81s2AC0UcWIYMGaL6+no1Nzfr97//vWbOnKmamprjhhY7lJeXh/XcBAIB5ebmOvbnAwjX2vadJOmff9xfacmnxbg1iIW9Bw5p+ZY9ajn6WQC6W8SBpWfPnho0aJAkafTo0frzn/+sBQsW6De/+c0xx2ZlZWnfvvBu4n379ikrK8t63nwsOzs77JhRo0Ydtw0+n08+ny/SpgOwiTkS7v8be55y00+PbWMQE3/e+ZWWb9mj6EZFAscX9ToswWAwbDxJRwUFBaqqqgp7bN26ddaYl7y8PGVlZYUdEwgEtGnTpuOOiwEQfwzxLZXovJ7v/9tOYoFNIuphKS8vV3Fxsfr376+DBw9qyZIlqq6u1tq1ayVJM2bM0DnnnKOKigpJ0i233KIrr7xSjz/+uCZNmqSlS5dq8+bNeuaZZyRJHo9Hc+fO1UMPPaTBgwcrLy9P9957r3JyclRSUtK97xSAbYJHv6M8nti2A7HjOfqXHySwwCYRBZYvvvhCM2bMUFNTk9LS0jRixAitXbtWV199tSSpsbFRXm+o0+bSSy/VkiVLdM899+juu+/W4MGDtWLFCg0bNsw65s4771Rra6tuvPFGHThwQJdffrnWrFmjXr16ddNbBGC7o99RXhJLwkoyA0swxg2Ba0W9Dks8YB0WILYG//tqHWk3VFv+E2WnJce6OYiBj3Y3a/J/bVB2Wi/Vlo+PdXPwAxHJ9zd7CQGImvlrj0f0sCQqs3ONkhDsQmABEDXzK4qKUOLyWmNYYtwQuBaBBUDUzMoyeSVxJR2dJuSCUQaIUwQWAFGzvqJILAnLmtZMFwtsQmABEDWDWUIJz0NJCDYjsACISscSAHElcZklIQbdwi4EFgBR6fj95KGHJWGZJaEgXSywCYEFQFQ6fj0RVxIXs4RgNwILgKiElYRILAnLS0kINiOwAIhKeA8LiSVReVk4DjYjsACIikFNCKIkBPsRWABEpeNv1F4CS8JiaX7YjcACoNswSyhxmbs1Gwar3cIeBBYAUQmb1hy7ZiDGOi4aSFkIdiCwAIiKIWYJ4W8DC4kF3Y/AAiAq4T0sJJZE5e3wbUJggR0ILACiEjZJiLySsMJ6WIIxbAhci8ACICr8Ng2JkhDsR2ABEJWO303s1py4KAnBbgQWANEJ2/wwds1AbFESgt0ILACiEjZLKIbtQGxREoLdCCwAohI2S4guloTVcZVjAgvsQGABEBW2EoL0fVgNLc8f27bAnQgsAKLScRl2OlgSW2gDRBILuh+BBUBUgpSEcJSXDRBhIwILgKiYg27JKgj1sMS4IXAlAguA6Bz9ciKvwAosJBbYgMACICrmVxPlIFASgp0ILACiYtDDgqMoCcFOBBYAUWEMC0zeo10s7SQW2IDAAiAqoR4WEkuiM0tCBiUh2CCiwFJRUaExY8YoJSVFGRkZKikp0bZt2054zrhx444uKBR+mzRpknXMrFmzjnl+4sSJp/aOADgqSE0IR1ESgp16RHJwTU2NSktLNWbMGH333Xe6++67NWHCBH3yySc644wzOj3njTfe0OHDh637+/fv18iRIzV16tSw4yZOnKgXXnjBuu/z+SJpGoAYMfOKl8CS8MySEINuYYeIAsuaNWvC7ldWViojI0N1dXUaO3Zsp+ekp6eH3V+6dKlOP/30YwKLz+dTVlZWJM0BEEcoCcEMrYxhgR2iGsPS3Nws6dhQciLPPfecpk2bdkyPTHV1tTIyMjRkyBDdfPPN2r9//3Ffo62tTYFAIOwGIDasihB5JeGZJSE6WGCHUw4swWBQc+fO1WWXXaZhw4Z16Zz3339fW7du1fXXXx/2+MSJE/XSSy+pqqpKjzzyiGpqalRcXKz29vZOX6eiokJpaWnWLTc391TfBoAoWbOEYtwOxB57CcFOEZWEOiotLdXWrVu1YcOGLp/z3HPPafjw4brkkkvCHp82bZr1/8OHD9eIESN03nnnqbq6WuPHjz/mdcrLy1VWVmbdDwQChBYgRkI9LESWROc9+itwO4EFNjilHpY5c+Zo1apVevfdd9WvX78undPa2qqlS5dq9uzZJz124MCB6tOnj3bs2NHp8z6fT6mpqWE3ALFh/jZNXEGoJERgQfeLqIfFMAz94he/0PLly1VdXa28vLwun7ts2TK1tbXpn//5n0967O7du7V//35lZ2dH0jwAMWB9NZFYEh7TmmGniHpYSktL9fLLL2vJkiVKSUmR3++X3+/Xt99+ax0zY8YMlZeXH3Puc889p5KSEp199tlhj7e0tOiOO+7Qxo0btXPnTlVVVWnKlCkaNGiQioqKTvFtAXBKaFoziSXRMUsIdoqoh2XRokWSvl8MrqMXXnhBs2bNkiQ1NjbK6w3PQdu2bdOGDRv09ttvH/OaSUlJ+vDDD/Xiiy/qwIEDysnJ0YQJE/SrX/2KtViAHwSW5sf3GHQLO0VcEjqZ6urqYx4bMmTIcc9NTk7W2rVrI2kGgDjCQrcwMa0ZdmIvIQBRMb+bmCUEVrqFnQgsAKJCDwtMjGGBnQgsAKJiTWsmsSQ8SkKwE4EFQFRYOA4mSkKwE4EFQFRYmh8mSkKwE4EFQFTY/BAmFo6DnQgsALqFhz6WhGf2sLA0P+xAYAEQFXpYYDJ7WNj8EHYgsACICmNYYKIkBDsRWABEJcgsIRxl7spCSQh2ILAAiIrBOiw4ir2EYCcCC4CohJbmj2kzEAesMSzBGDcErkRgARCV0NL8JJZEZ84SoocFdiCwAIgSJSF8L8lrLs1PYEH3I7AAiAqbH8LkoSQEGxFYAESFWUIwURKCnQgsAKJizRKKcTsQe6Hdmgks6H4EFgBRYZYQTOZuzWx+CDsQWABExaAkhKNY6RZ2IrAAiApL88PEGBbYicACIDpsfoijkqwxLDFuCFyJwAIgKtYYFvpYEp6H3ZphIwILgKgE2UsIR1ESgp0ILACiwncTTKGVbmPcELgSgQVAVMzvJi9dLAkvtNItiQXdj8ACICoGJSEcRUkIdiKwAIgKC8fBxDossBOBBUB0rM0PSSyJzhzDEiSxwAYEFgBRsRaOI68kPA8lIdiIwAIgKsHg9/8lr4CSEOxEYAEQFeu7iS6WhBea1kxiQfeLKLBUVFRozJgxSklJUUZGhkpKSrRt27YTnlNZWSmPxxN269WrV9gxhmFo3rx5ys7OVnJysgoLC7V9+/bI3w0Ax5lfTl7ySsIzMyvTmmGHiAJLTU2NSktLtXHjRq1bt05HjhzRhAkT1NraesLzUlNT1dTUZN0+//zzsOfnz5+vp59+WosXL9amTZt0xhlnqKioSIcOHYr8HQFwVGhpfiQ6SkKwU49IDl6zZk3Y/crKSmVkZKiurk5jx4497nkej0dZWVmdPmcYhp566indc889mjJliiTppZdeUmZmplasWKFp06ZF0kQADjOszQ+JLIkuyQosJBZ0v6jGsDQ3N0uS0tPTT3hcS0uLBgwYoNzcXE2ZMkUff/yx9VxDQ4P8fr8KCwutx9LS0pSfn6/a2tpOX6+trU2BQCDsBiBWjs4SinErEHssHAc7nXJgCQaDmjt3ri677DINGzbsuMcNGTJEzz//vFauXKmXX35ZwWBQl156qXbv3i1J8vv9kqTMzMyw8zIzM63n/lZFRYXS0tKsW25u7qm+DQBRClo9LLFtB2LPQw8LbHTKgaW0tFRbt27V0qVLT3hcQUGBZsyYoVGjRunKK6/UG2+8ob59++o3v/nNqf7RKi8vV3Nzs3XbtWvXKb8WgOgYLByHoxjDAjtFNIbFNGfOHK1atUrr169Xv379Ijr3tNNO00UXXaQdO3ZIkjW2Zd++fcrOzraO27dvn0aNGtXpa/h8Pvl8vlNpOoBuZoSWukWCSzr6KzAr3cIOEfWwGIahOXPmaPny5XrnnXeUl5cX8R/Y3t6ujz76yAoneXl5ysrKUlVVlXVMIBDQpk2bVFBQEPHrA3CW2cPCtGZQEoKdIuphKS0t1ZIlS7Ry5UqlpKRYY0zS0tKUnJwsSZoxY4bOOeccVVRUSJIefPBB/fjHP9agQYN04MABPfroo/r88891/fXXS/r+Az537lw99NBDGjx4sPLy8nTvvfcqJydHJSUl3fhWAdghNK2ZxJLoKAnBThEFlkWLFkmSxo0bF/b4Cy+8oFmzZkmSGhsb5fWGOm6+/vpr3XDDDfL7/TrrrLM0evRovffee7rgggusY+688061trbqxhtv1IEDB3T55ZdrzZo1xywwByD+mAvHMegWVkmIHhbYIKLA0pXllqurq8PuP/nkk3ryySdPeI7H49GDDz6oBx98MJLmAIgjBBZYPSx0scAG7CUEICrmb9OUhOChJAQbEVgARMVgHRYclcTCcbARgQVAVFiaHyavl1lCsA+BBUBU2PwQJqskFIxxQ+BKBBYAUWGWEEzsJQQ7EVgARIUeFpjYrRl2IrAAiA5jWHAUC8fBTgQWAFEJTWtGovNQEoKNCCwAomKVhEgsCS/JSw8L7ENgARAVpjXDxEq3sBOBBUBUDFESwvcoCcFOBBYAUWGlW5iSWDgONiKwAIhKaFoziSXReVk4DjYisACIDgvH4SgWjoOdCCwAohKkJISjPCwcBxsRWABExVqan5JQwjNXum0nr8AGBBYAUWEdFpi8R79RDHpYYAMCC4CosA4LTJSEYCcCC4CosPkhTEnMEoKNCCwAomIwSwhHeelhgY0ILACiYpWEYtsMxAGmNcNOBBYAUbGW5qeLJeF52fwQNiKwAIgKPSwwsfkh7ERgARCV0LRmIkuioyQEOxFYAESFzQ9hCk1rjnFD4EoEFgBRscawxLgdiD1zt+Z2EgtsQGABEBV6WGAyS0KsdAs7EFgARIW9hGDyUhKCjQgsAKJCDwtMLBwHOxFYAESFzQ9hMjc/JLDADgQWAFFh80OYKAnBThEFloqKCo0ZM0YpKSnKyMhQSUmJtm3bdsJzfvvb3+qKK67QWWedpbPOOkuFhYV6//33w46ZNWuWPB5P2G3ixImRvxsAjmOWEEyUhGCniAJLTU2NSktLtXHjRq1bt05HjhzRhAkT1NraetxzqqurNX36dL377ruqra1Vbm6uJkyYoD179oQdN3HiRDU1NVm3V1555dTeEQBHMYYFJnOWENOaYYcekRy8Zs2asPuVlZXKyMhQXV2dxo4d2+k5v/vd78LuP/vss3r99ddVVVWlGTNmWI/7fD5lZWVF0hwAccAaw0IfS8Ize1joYIEdohrD0tzcLElKT0/v8jnffPONjhw5csw51dXVysjI0JAhQ3TzzTdr//79x32NtrY2BQKBsBuA2LCmNZNXEh4lIdjplANLMBjU3Llzddlll2nYsGFdPu+Xv/ylcnJyVFhYaD02ceJEvfTSS6qqqtIjjzyimpoaFRcXq729vdPXqKioUFpamnXLzc091bcBIEpsfgiTOUuIkhDsEFFJqKPS0lJt3bpVGzZs6PI5Dz/8sJYuXarq6mr16tXLenzatGnW/w8fPlwjRozQeeedp+rqao0fP/6Y1ykvL1dZWZl1PxAIEFqAGLEG3dLFkvAoCcFOp9TDMmfOHK1atUrvvvuu+vXr16VzHnvsMT388MN6++23NWLEiBMeO3DgQPXp00c7duzo9Hmfz6fU1NSwG4DYYNAtTJSEYKeIelgMw9AvfvELLV++XNXV1crLy+vSefPnz9d//Md/aO3atbr44otPevzu3bu1f/9+ZWdnR9I8ADHAoFuYWDgOdoqoh6W0tFQvv/yylixZopSUFPn9fvn9fn377bfWMTNmzFB5ebl1/5FHHtG9996r559/Xueee651TktLiySppaVFd9xxhzZu3KidO3eqqqpKU6ZM0aBBg1RUVNRNbxOAXehhganjwnFsgIjuFlFgWbRokZqbmzVu3DhlZ2dbt1dffdU6prGxUU1NTWHnHD58WP/4j/8Yds5jjz0mSUpKStKHH36ov//7v9f555+v2bNna/To0frf//1f+Xy+bnqbAOwS2vwQic7bIbWSV9DdIi4JnUx1dXXY/Z07d57w+OTkZK1duzaSZgCII+wlBFNShw9B0DDkJcaiG7GXEICohNZh4csp0Xk6fKO008WCbkZgARAVxrDAREkIdiKwAIgKs4Rg8nb4CDBTCN2NwAIgKvSwwOQNG8MSw4bAlQgsAKJirXQb43Yg9joGFpbnR3cjsACICj0sMHUsCbEOC7obgQVAVELrsJBYEl2Sl5IQ7ENgARAV1mGByUNJCDYisACISqgkRGJBqCxESQjdjcACICoMukVHZlmIDhZ0NwILgKgw6BYdmT1trHSL7kZgARAVFo5DR2ZJKEgXC7oZgQVAVEJ7CcW4IYgL5losdLCguxFYAETFKgnFthmIE+aOzSzNj+5GYAEQFcawoCPzc8AYFnQ3AguAqFizhEgskOT1miUhAgu6F4EFQFToYUFHoZJQjBsC1yGwAIgKs4TQkTWtmcSCbkZgARCVILOE0IE1rZmSELoZgQVAdJglhA6SvExrhj0ILACiwuaH6MhLSQg2IbAAiIq1cBx9LFAouFISQncjsACICj0s6MjLLCHYhMACICqhac0kFnQcw0JiQfcisACISmhaM9BhpVu6WNDNCCwAosK0ZnRESQh2IbAAiA7TmtFBkoeSEOxBYAEQFfYSQkdsfgi7EFgARIW9hNARJSHYhcACICrMEkJH5iwh1mFBdyOwAIiKVRKKcTsQH6y9hOhiQTeLKLBUVFRozJgxSklJUUZGhkpKSrRt27aTnrds2TINHTpUvXr10vDhw7V69eqw5w3D0Lx585Sdna3k5GQVFhZq+/btkb0TADFBSQgdeSgJwSYRBZaamhqVlpZq48aNWrdunY4cOaIJEyaotbX1uOe89957mj59umbPnq0tW7aopKREJSUl2rp1q3XM/Pnz9fTTT2vx4sXatGmTzjjjDBUVFenQoUOn/s4AOCJozRIisYDdmmGfHpEcvGbNmrD7lZWVysjIUF1dncaOHdvpOQsWLNDEiRN1xx13SJJ+9atfad26dfqv//ovLV68WIZh6KmnntI999yjKVOmSJJeeuklZWZmasWKFZo2bdqpvC8AjmEdFoSw0i3sElFg+VvNzc2SpPT09OMeU1tbq7KysrDHioqKtGLFCklSQ0OD/H6/CgsLrefT0tKUn5+v2traTgNLW1ub2trarPuBQCCat3Fc37UH9R+r/2LLa9stO62X/vWyPPVIYpgS7GWwDgs6MEtCr/55lzY1fBXj1qA79fB69O+TLojdn3+qJwaDQc2dO1eXXXaZhg0bdtzj/H6/MjMzwx7LzMyU3++3njcfO94xf6uiokIPPPDAqTa9y4KG9MKfdtr+59hlWE6aLh3UJ9bNgMux+SE6Su11miTp3W1fStu+jHFr0J169vD+MANLaWmptm7dqg0bNnRne7qkvLw8rNcmEAgoNze32/8cr0cqveq8bn9du71et0f+wCG1tH0X66YgARgGC8ch5N8n/Z0uyElVezAY66agmyV5Y9tjf0qBZc6cOVq1apXWr1+vfv36nfDYrKws7du3L+yxffv2KSsry3refCw7OzvsmFGjRnX6mj6fTz6f71SaHpEeSV7dUTTU9j+nu236v6/kDxxilD4cweaH6Civzxkqu/r8WDcDLhRRXDIMQ3PmzNHy5cv1zjvvKC8v76TnFBQUqKqqKuyxdevWqaCgQJKUl5enrKyssGMCgYA2bdpkHYPIhFaaJLHAfiwcB8AJEfWwlJaWasmSJVq5cqVSUlKsMSZpaWlKTk6WJM2YMUPnnHOOKioqJEm33HKLrrzySj3++OOaNGmSli5dqs2bN+uZZ56R9P0/cnPnztVDDz2kwYMHKy8vT/fee69ycnJUUlLSjW81cXiYVggHWbs1x7gdANwtosCyaNEiSdK4cePCHn/hhRc0a9YsSVJjY6O8Hepcl156qZYsWaJ77rlHd999twYPHqwVK1aEDdS988471draqhtvvFEHDhzQ5ZdfrjVr1qhXr16n+LYSW2hp7Bg3BAmFDhYAdooosHRlXn11dfUxj02dOlVTp0497jkej0cPPvigHnzwwUiag+OwSkIkFjiAlW4BOIFFOlyIkhCcZO4l5CWxALARgcWF2N4dTiIXA3ACgcWF2N4dTmKWEAAnEFhciO3d4SRmCQFwAoHFhdjeHU5iaX4ATiCwuFASC8fBSdbmhyQWAPYhsLiQuQwOgQVOMGcJ0cMCwE4EFhfysA4LHGTmYi+BBYCNCCwulMQYFjgo9DEjsQCwD4HFhbwsHAcHmStgUxICYCcCiwuxWzOcFLQG3QKAfQgsLsS0ZjgpNK2ZyALAPgQWF0pilhCcxMJxABxAYHEhdmuGk1g4DoATCCwuREkITgpNayaxALAPgcWFKAnBSYYYdQvAfgQWF6IkBCcZ5BUADiCwuJCXkhAcZE1rpiQEwEYEFhdiHRY4yWCWEAAHEFhcyFzptp3AAgfRwQLATgQWF/IeTSzkFTghNIaFxALAPgQWFzJ/02XQLZxgzhJit2YAdiKwuBC7NcNJVk8egQWAjQgsLsSgWzgpaA26JbEAsA+BxYXMrnkCC5zA0vwAnEBgcSFz0C2BBY5g4TgADiCwuJBZEmoPxrghSAihHhYiCwD7EFhcyCwJGfSwwAHWwnHkFQA2IrC4kIdBt3CQ+SljWjMAOxFYXCjJS0kIzgnlYhILAPsQWFyIkhCcFKQkBMABEQeW9evXa/LkycrJyZHH49GKFStOePysWbPk8XiOuV144YXWMffff/8xzw8dOjTiN4PvsQ4LnGQwSwiAAyIOLK2trRo5cqQWLlzYpeMXLFigpqYm67Zr1y6lp6dr6tSpYcddeOGFYcdt2LAh0qbhKC8r3SIGmCUEwE49Ij2huLhYxcXFXT4+LS1NaWlp1v0VK1bo66+/1nXXXRfekB49lJWVFWlz0Al2a4aTrFlCMW4HAHdzfAzLc889p8LCQg0YMCDs8e3btysnJ0cDBw7Utddeq8bGxuO+RltbmwKBQNgNIaHdmgkssB8r3QJwgqOBZe/evXrrrbd0/fXXhz2en5+vyspKrVmzRosWLVJDQ4OuuOIKHTx4sNPXqaiosHpu0tLSlJub60TzfzCskhCzhOAAMxd7SSwAbORoYHnxxRfVu3dvlZSUhD1eXFysqVOnasSIESoqKtLq1at14MABvfbaa52+Tnl5uZqbm63brl27HGj9D4e10i09LHCAIT5nAOwX8RiWU2UYhp5//nn9y7/8i3r27HnCY3v37q3zzz9fO3bs6PR5n88nn89nRzNdgWnNcJI5uJsOFgB2cqyHpaamRjt27NDs2bNPemxLS4s+++wzZWdnO9Ay92GWEJwUmtZMYgFgn4gDS0tLi+rr61VfXy9JamhoUH19vTVItry8XDNmzDjmvOeee075+fkaNmzYMc/dfvvtqqmp0c6dO/Xee+/ppz/9qZKSkjR9+vRImweFBt22k1jgCBaOA2C/iEtCmzdv1lVXXWXdLysrkyTNnDlTlZWVampqOmaGT3Nzs15//XUtWLCg09fcvXu3pk+frv3796tv3766/PLLtXHjRvXt2zfS5kGhkhALx8EJBiUhAA6IOLCMGzfuhGMjKisrj3ksLS1N33zzzXHPWbp0aaTNwAmYJSHyCpwQ2vyQxALAPuwl5EJmSYgeFjiBheMAOIHA4kLWSreMYYEDmCUEwAkEFheiJAQnhUrEJBYA9iGwuBC7NcNJLM0PwAkEFhdi80M4ylqHBQDsQ2BxIRaOg5NCPSxEFgD2IbC4kPfo3ypL88MJ5ufMS14BYCMCiwtZmx/SxQIHhIbcklgA2IfA4kKUhOAkc3A3FSEAdiKwuFBoWjOJBfbjYwbACQQWFzLHsDCtGU5gWjMAJxBYXIgxLHCUtdItiQWAfQgsLsRKt3CSIfYSAmA/AosLJVESgoPMjxm7NQOwE4HFhcyueVa6hRMYwwLACQQWF7KmNQdj3BAkBGtac4zbAcDdCCwuZK44yrRmOIHNmgE4gcDiQl5KQogBVroFYCcCiwux0i2c0rEXjzEsAOxEYHEhNj+EUzp+xMgrAOxEYHGhJHpY4JCOHzGmNQOwE4HFhTysdAuHBCkJAXAIgcWFzFlCLBwHu4WXhEgsAOxDYHGhJC9L88MZhhjEAsAZBBYXYvNDOCWsh4XAAsBGBBYX8lASQgyQVwDYicDiQuzWDKeE97AQWQDYh8DiQuYYFla6hd06jmHxklcA2IjA4kKUhOCUILOEADiEwOJCHUtCrHYLO7E0PwCnEFhcKKnDNwd5BXbi4wXAKREHlvXr12vy5MnKycmRx+PRihUrTnh8dXW1PB7PMTe/3x923MKFC3XuueeqV69eys/P1/vvvx9p03BUxyXSGccCOzGtGYBTIg4sra2tGjlypBYuXBjRedu2bVNTU5N1y8jIsJ579dVXVVZWpvvuu08ffPCBRo4cqaKiIn3xxReRNg+SPB3+VhnHAlsxhgWAQ3pEekJxcbGKi4sj/oMyMjLUu3fvTp974okndMMNN+i6666TJC1evFhvvvmmnn/+ed11110R/1mJjpIQnMIsIQBOcWwMy6hRo5Sdna2rr75af/rTn6zHDx8+rLq6OhUWFoYa5fWqsLBQtbW1nb5WW1ubAoFA2A0hYSUhVruFjViHBYBTbA8s2dnZWrx4sV5//XW9/vrrys3N1bhx4/TBBx9Ikv7617+qvb1dmZmZYedlZmYeM87FVFFRobS0NOuWm5tr99v4Qen4vUFJCHYK2605hu0A4H4Rl4QiNWTIEA0ZMsS6f+mll+qzzz7Tk08+qf/5n/85pdcsLy9XWVmZdT8QCBBaOujYw0IHC+zU8eNFBwsAO9keWDpzySWXaMOGDZKkPn36KCkpSfv27Qs7Zt++fcrKyur0fJ/PJ5/PZ3s7f6iSOgwmCJJYYCNKQgCcEpN1WOrr65WdnS1J6tmzp0aPHq2qqirr+WAwqKqqKhUUFMSieT94XkpCcIjBSiwAHBJxD0tLS4t27Nhh3W9oaFB9fb3S09PVv39/lZeXa8+ePXrppZckSU899ZTy8vJ04YUX6tChQ3r22Wf1zjvv6O2337Zeo6ysTDNnztTFF1+sSy65RE899ZRaW1utWUOIjIeSEJxy9PNF5woAu0UcWDZv3qyrrrrKum+OJZk5c6YqKyvV1NSkxsZG6/nDhw/rtttu0549e3T66adrxIgR+uMf/xj2Gtdcc42+/PJLzZs3T36/X6NGjdKaNWuOGYiLrkvyetQeNFiaH7YyP11eEgsAm3kMF3yjBQIBpaWlqbm5WampqbFuTlwY/O+rdaTdUG35T5Sdlhzr5sCl/M2H9OOKKvXwerTj1/8v1s0B8AMTyfc3ewm5lFkWoiQEO5ljpOhgAWA3AotLmavdMksIdjI/XSzLD8BuBBaXMmcKMUsIdrIqyuQVADYjsLiUl5IQHEBeAeAUAotLeehhgYMYwwLAbgQWlzJXu2UMC+xk5mGmNQOwG4HFpSgJwQnWLKEYtwOA+xFYXCo0rZnEAvtYs4ToYQFgMwKLSyUd/ZslsMBOBj0sABxCYHEpqyQUjHFD4GpWHCaxALAZgcWlvJSE4ACmNQNwCoHFpbyUhOAIc2l+IgsAexFYXIoeFjghNK05tu0A4H4EFpdiWjOcYH6+6GEBYDcCi0tZK92SWGAjQ8wSAuAMAotLmbs1t1MSgo2sQbckFgA2I7C4lFkSIq/ATgbzmgE4hMDiUmx+CCdYJSHyCgCbEVhcytr8kLwCG7EOCwCnEFhcKrTSLYkF9mO3ZgB2I7C4lJeSEBxg7dZMXgFgMwKLS3kpCcEBlIQAOIXA4lJmF307iQU2Mj9dLBwHwG4EFpcyS0IGJSHYiM8XAKcQWFzKw9L8cECohyWmzQCQAAgsLsVKt3ACK90CcAqBxaW8R/9m6bKHnczPF9OaAdiNwOJSod2aCSywj1USimkrACQCAotLhRaOi3FD4GqhkhCRBYC9CCwuZc4SYgwL7GSWhIgrAOxGYHGp0G7NBBbYh82aATgl4sCyfv16TZ48WTk5OfJ4PFqxYsUJj3/jjTd09dVXq2/fvkpNTVVBQYHWrl0bdsz9998vj8cTdhs6dGikTUMHrHQLJ7DSLQCnRBxYWltbNXLkSC1cuLBLx69fv15XX321Vq9erbq6Ol111VWaPHmytmzZEnbchRdeqKamJuu2YcOGSJuGDqySEIkFNjLELCEAzugR6QnFxcUqLi7u8vFPPfVU2P1f//rXWrlypf7whz/ooosuCjWkRw9lZWVF2hwcByUhOIF1WAA4xfExLMFgUAcPHlR6enrY49u3b1dOTo4GDhyoa6+9Vo2Njcd9jba2NgUCgbAbwnlZ6RYOCJWESCwA7OV4YHnsscfU0tKin//859Zj+fn5qqys1Jo1a7Ro0SI1NDToiiuu0MGDBzt9jYqKCqWlpVm33Nxcp5r/g2GOYaEkBDuZJSF6WADYzdHAsmTJEj3wwAN67bXXlJGRYT1eXFysqVOnasSIESoqKtLq1at14MABvfbaa52+Tnl5uZqbm63brl27nHoLPxjmGBYWjoOd+HgBcErEY1hO1dKlS3X99ddr2bJlKiwsPOGxvXv31vnnn68dO3Z0+rzP55PP57Ojma4RGsMS44bA1UKbH9LFAsBejvSwvPLKK7ruuuv0yiuvaNKkSSc9vqWlRZ999pmys7MdaJ07sTQ/nMDCcQCcEnEPS0tLS1jPR0NDg+rr65Wenq7+/furvLxce/bs0UsvvSTp+zLQzJkztWDBAuXn58vv90uSkpOTlZaWJkm6/fbbNXnyZA0YMEB79+7Vfffdp6SkJE2fPr073mNCYqVbOMH8dHlZghKAzSL+Z2bz5s266KKLrCnJZWVluuiiizRv3jxJUlNTU9gMn2eeeUbfffedSktLlZ2dbd1uueUW65jdu3dr+vTpGjJkiH7+85/r7LPP1saNG9W3b99o31/CoiQEJ4R6WOhjAWCviHtYxo0bd8K1PSorK8PuV1dXn/Q1ly5dGmkzcBLWSrfMEoKNWIcFgFPoyHUpSkJwAkvzA3AKgcWlWDgOTghtfkhkAWAvAotLmT0sLM0POzFLCIBTCCwuFdqtmcAC+4TWYYlpMwAkAAKLS5klofZgjBsCVzN7WNitGYDdCCwuRUkITmDQLQCnEFhcipIQnEBJCIBTCCwuRUkITgj1sJBYANiLwOJS7NYMJxiiJgTAGQQWl0qyluYnsMA+jGEB4BQCi0t5zJIQgQU2YgwLAKcQWFyKlW7hBKY1A3AKgcWlmNYMJ7D5IQCnEFhcKrRbc4wbAlczB90ySwiA3QgsLuVlDAscQA8LAKcQWFyKac1wAh8vAE4hsLhUktec1hzjhsDVQrOE6GIBYC8Ci0tZ05qZJgQbmYO6iSsA7EZgcSlKQnCC+fHyklgA2IzA4lKUhOAEa5YQJSEANiOwuBQlITiBpfkBOIXA4lKUhOAEluYH4BQCi0uxND+cEMrDJBYA9iKwuBS7NcMJoTEsMW4IANcjsLiU+QXCSrewU5AxLAAcQmBxKUpCcAS7NQNwCIHFpULTmkkssA+DbgE4hcDiUlZJiC4W2IjNDwE4hcDiUqGSEIEF9gktzU9iAWAvAotLmSUhOlhgJ2Y1A3AKgcWlrIXjSCywESvdAnBKxIFl/fr1mjx5snJycuTxeLRixYqTnlNdXa0f/ehH8vl8GjRokCorK485ZuHChTr33HPVq1cv5efn6/3334+0aejAQ0kIDggySwiAQyIOLK2trRo5cqQWLlzYpeMbGho0adIkXXXVVaqvr9fcuXN1/fXXa+3atdYxr776qsrKynTffffpgw8+0MiRI1VUVKQvvvgi0ubhKKY1w0nkFQB26xHpCcXFxSouLu7y8YsXL1ZeXp4ef/xxSdLf/d3facOGDXryySdVVFQkSXriiSd0ww036LrrrrPOefPNN/X888/rrrvuirSJkJR0NIoyrRl2oiQEwCkRB5ZI1dbWqrCwMOyxoqIizZ07V5J0+PBh1dXVqby83Hre6/WqsLBQtbW1nb5mW1ub2trarPuBQKD7G/4DZ5aEdn/9rR74w8cxbg3cauueZkmhzxsA2MX2wOL3+5WZmRn2WGZmpgKBgL799lt9/fXXam9v7/SYTz/9tNPXrKio0AMPPGBbm90gtddpkqT9rYf1wp92xrYxcL2UXrb/UwIgwf0g/5UpLy9XWVmZdT8QCCg3NzeGLYo/F+X21q9/Olx7DnwT66bA5Xr1SNLPx/DzB8BetgeWrKws7du3L+yxffv2KTU1VcnJyUpKSlJSUlKnx2RlZXX6mj6fTz6fz7Y2u4HX69E/5fePdTMAAOgWtq/DUlBQoKqqqrDH1q1bp4KCAklSz549NXr06LBjgsGgqqqqrGMAAEBiiziwtLS0qL6+XvX19ZK+n7ZcX1+vxsZGSd+Xa2bMmGEdf9NNN+n//u//dOedd+rTTz/Vf//3f+u1117Trbfeah1TVlam3/72t3rxxRf1l7/8RTfffLNaW1utWUMAACCxRVwS2rx5s6666irrvjmWZObMmaqsrFRTU5MVXiQpLy9Pb775pm699VYtWLBA/fr107PPPmtNaZaka665Rl9++aXmzZsnv9+vUaNGac2aNccMxAUAAInJY7hgoY5AIKC0tDQ1NzcrNTU11s0BAABdEMn3N3sJAQCAuEdgAQAAcY/AAgAA4h6BBQAAxD0CCwAAiHsEFgAAEPcILAAAIO4RWAAAQNwjsAAAgLhn+27NTjAX6w0EAjFuCQAA6Crze7sri+67IrAcPHhQkpSbmxvjlgAAgEgdPHhQaWlpJzzGFXsJBYNB7d27VykpKfJ4PN362oFAQLm5udq1axf7FJ0E1yoyXK+u41pFhuvVdVyrrrPjWhmGoYMHDyonJ0de74lHqbiih8Xr9apfv362/hmpqal8mLuIaxUZrlfXca0iw/XqOq5V13X3tTpZz4qJQbcAACDuEVgAAEDcI7CchM/n03333SefzxfrpsQ9rlVkuF5dx7WKDNer67hWXRfra+WKQbcAAMDd6GEBAABxj8ACAADiHoEFAADEPQILAACIewSWk1i4cKHOPfdc9erVS/n5+Xr//fdj3aSYu//+++XxeMJuQ4cOtZ4/dOiQSktLdfbZZ+vMM8/UP/zDP2jfvn0xbLFz1q9fr8mTJysnJ0cej0crVqwIe94wDM2bN0/Z2dlKTk5WYWGhtm/fHnbMV199pWuvvVapqanq3bu3Zs+erZaWFgffhXNOdr1mzZp1zGdt4sSJYcckyvWqqKjQmDFjlJKSooyMDJWUlGjbtm1hx3TlZ6+xsVGTJk3S6aefroyMDN1xxx367rvvnHwrtuvKtRo3btwxn62bbrop7JhEuFaLFi3SiBEjrMXgCgoK9NZbb1nPx9NnisByAq+++qrKysp033336YMPPtDIkSNVVFSkL774ItZNi7kLL7xQTU1N1m3Dhg3Wc7feeqv+8Ic/aNmyZaqpqdHevXv1s5/9LIatdU5ra6tGjhyphQsXdvr8/Pnz9fTTT2vx4sXatGmTzjjjDBUVFenQoUPWMddee60+/vhjrVu3TqtWrdL69et14403OvUWHHWy6yVJEydODPusvfLKK2HPJ8r1qqmpUWlpqTZu3Kh169bpyJEjmjBhglpbW61jTvaz197erkmTJunw4cN677339OKLL6qyslLz5s2LxVuyTVeulSTdcMMNYZ+t+fPnW88lyrXq16+fHn74YdXV1Wnz5s36yU9+oilTpujjjz+WFGefKQPHdckllxilpaXW/fb2diMnJ8eoqKiIYati77777jNGjhzZ6XMHDhwwTjvtNGPZsmXWY3/5y18MSUZtba1DLYwPkozly5db94PBoJGVlWU8+uij1mMHDhwwfD6f8corrxiGYRiffPKJIcn485//bB3z1ltvGR6Px9izZ49jbY+Fv71ehmEYM2fONKZMmXLccxL5en3xxReGJKOmpsYwjK797K1evdrwer2G3++3jlm0aJGRmppqtLW1OfsGHPS318owDOPKK680brnlluOek6jXyjAM46yzzjKeffbZuPtM0cNyHIcPH1ZdXZ0KCwutx7xerwoLC1VbWxvDlsWH7du3KycnRwMHDtS1116rxsZGSVJdXZ2OHDkSdt2GDh2q/v37J/x1a2hokN/vD7s2aWlpys/Pt65NbW2tevfurYsvvtg6prCwUF6vV5s2bXK8zfGgurpaGRkZGjJkiG6++Wbt37/fei6Rr1dzc7MkKT09XVLXfvZqa2s1fPhwZWZmWscUFRUpEAhYv1G70d9eK9Pvfvc79enTR8OGDVN5ebm++eYb67lEvFbt7e1aunSpWltbVVBQEHefKVdsfmiHv/71r2pvbw/7S5CkzMxMffrppzFqVXzIz89XZWWlhgwZoqamJj3wwAO64oortHXrVvn9fvXs2VO9e/cOOyczM1N+vz82DY4T5vvv7DNlPuf3+5WRkRH2fI8ePZSenp6Q12/ixIn62c9+pry8PH322We6++67VVxcrNraWiUlJSXs9QoGg5o7d64uu+wyDRs2TJK69LPn9/s7/fyZz7lRZ9dKkv7pn/5JAwYMUE5Ojj788EP98pe/1LZt2/TGG29ISqxr9dFHH6mgoECHDh3SmWeeqeXLl+uCCy5QfX19XH2mCCyIWHFxsfX/I0aMUH5+vgYMGKDXXntNycnJMWwZ3GbatGnW/w8fPlwjRozQeeedp+rqao0fPz6GLYut0tJSbd26NWzsGDp3vGvVcZzT8OHDlZ2drfHjx+uzzz7Teeed53QzY2rIkCGqr69Xc3Ozfv/732vmzJmqqamJdbOOQUnoOPr06aOkpKRjRkPv27dPWVlZMWpVfOrdu7fOP/987dixQ1lZWTp8+LAOHDgQdgzXTdb7P9FnKisr65hB3d99952++uqrhL9+kjRw4ED16dNHO3bskJSY12vOnDlatWqV3n33XfXr1896vCs/e1lZWZ1+/szn3OZ416oz+fn5khT22UqUa9WzZ08NGjRIo0ePVkVFhUaOHKkFCxbE3WeKwHIcPXv21OjRo1VVVWU9FgwGVVVVpYKCghi2LP60tLTos88+U3Z2tkaPHq3TTjst7Lpt27ZNjY2NCX/d8vLylJWVFXZtAoGANm3aZF2bgoICHThwQHV1ddYx77zzjoLBoPUPaiLbvXu39u/fr+zsbEmJdb0Mw9CcOXO0fPlyvfPOO8rLywt7vis/ewUFBfroo4/CQt66deuUmpqqCy64wJk34oCTXavO1NfXS1LYZysRrlVngsGg2tra4u8z1a1DeF1m6dKlhs/nMyorK41PPvnEuPHGG43evXuHjYZORLfddptRXV1tNDQ0GH/605+MwsJCo0+fPsYXX3xhGIZh3HTTTUb//v2Nd955x9i8ebNRUFBgFBQUxLjVzjh48KCxZcsWY8uWLYYk44knnjC2bNlifP7554ZhGMbDDz9s9O7d21i5cqXx4YcfGlOmTDHy8vKMb7/91nqNiRMnGhdddJGxadMmY8OGDcbgwYON6dOnx+ot2epE1+vgwYPG7bffbtTW1hoNDQ3GH//4R+NHP/qRMXjwYOPQoUPWayTK9br55puNtLQ0o7q62mhqarJu33zzjXXMyX72vvvuO2PYsGHGhAkTjPr6emPNmjVG3759jfLy8li8Jduc7Frt2LHDePDBB43NmzcbDQ0NxsqVK42BAwcaY8eOtV4jUa7VXXfdZdTU1BgNDQ3Ghx9+aNx1112Gx+Mx3n77bcMw4uszRWA5if/8z/80+vfvb/Ts2dO45JJLjI0bN8a6STF3zTXXGNnZ2UbPnj2Nc845x7jmmmuMHTt2WM9/++23xr/9278ZZ511lnH66acbP/3pT42mpqYYttg57777riHpmNvMmTMNw/h+avO9995rZGZmGj6fzxg/fryxbdu2sNfYv3+/MX36dOPMM880UlNTjeuuu844ePBgDN6N/U50vb755htjwoQJRt++fY3TTjvNGDBggHHDDTcc8wtDolyvzq6TJOOFF16wjunKz97OnTuN4uJiIzk52ejTp49x2223GUeOHHH43djrZNeqsbHRGDt2rJGenm74fD5j0KBBxh133GE0NzeHvU4iXKt//dd/NQYMGGD07NnT6Nu3rzF+/HgrrBhGfH2mPIZhGN3bZwMAANC9GMMCAADiHoEFAADEPQILAACIewQWAAAQ9wgsAAAg7hFYAABA3COwAACAuEdgAQAAcY/AAgAA4h6BBQAAxD0CCwAAiHsEFgAAEPf+fwVjo4kqXOVQAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def aggregate(output):\n",
    "    total = []\n",
    "    for i in range(len(output)):\n",
    "        prev = int(output[i][0])\n",
    "        for j in range(1, len(output[i])):\n",
    "            if output[i][j] != prev and output[i][j] != 0:\n",
    "                total.append(prev)\n",
    "                prev = int(output[i][j])\n",
    "        total.append(prev)\n",
    "    return total\n",
    "\n",
    "# test:\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels, labels_count in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels_count = torch.tensor(labels_count).to(device)\n",
    "        images = images.float()\n",
    "        outputs = model(images)\n",
    "        # print(outputs.shape)\n",
    "        # print(outputs)\n",
    "        outputs = F.softmax(outputs, dim=2)\n",
    "        input_lengths = torch.full((images.shape[0],), images.shape[1], dtype=torch.long)\n",
    "        loss = criterion(outputs.permute(1, 0, 2), labels.cpu(),  input_lengths.cpu(), labels_count.cpu())\n",
    "        _, predicted = torch.max(outputs.data, 2)\n",
    "        total += labels.size(0)\n",
    "        i = 1\n",
    "        print(predicted[i])\n",
    "        print(labels_count)\n",
    "        print(labels[labels_count[i-1]: labels_count[i-1]+labels_count[i]])\n",
    "        print(test_loader.dataset.sec_labels[i])\n",
    "        plt.plot(test_loader.dataset.sec_labels[i])\n",
    "        s = aggregate(predicted)\n",
    "        \n",
    "        print(s,)\n",
    "        print(labels.tolist())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
