{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def to_one_hot(mask: torch.tensor,\n",
    "               num_classes: int) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "        mask : shape [n_task, shot, h, w]\n",
    "        num_classes : Number of classes\n",
    "\n",
    "    returns :\n",
    "        one_hot_mask : shape [n_task, shot, num_class, h, w]\n",
    "    \"\"\"\n",
    "    n_tasks, shot, h, w = mask.size()\n",
    "    one_hot_mask = torch.zeros(n_tasks, shot, num_classes, h, w)\n",
    "    new_mask = mask.unsqueeze(2).clone()\n",
    "    new_mask[torch.where(new_mask == 255)] = 0  # Ignore_pixels are anyways filtered out in the losses\n",
    "    one_hot_mask.scatter_(2, new_mask, 1).long()\n",
    "    return one_hot_mask\n",
    "def intersectionAndUnion(output: torch.Tensor, target: torch.Tensor, num_classes: int, ignore_index=255) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Calculate intersection and union for single task and shot.\n",
    "\n",
    "    Args:\n",
    "        output (torch.Tensor): Predicted tensor.\n",
    "        target (torch.Tensor): Ground truth tensor.\n",
    "        num_classes (int): Number of classes.\n",
    "        ignore_index (int, optional): Index to ignore in evaluation. Defaults to 255.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Intersection, Union, Target area.\n",
    "    \"\"\"\n",
    "    ignore_mask = target != ignore_index\n",
    "    output = output[ignore_mask]\n",
    "    target = target[ignore_mask]\n",
    "\n",
    "    intersection = torch.zeros(num_classes)\n",
    "    union = torch.zeros(num_classes)\n",
    "    target_area = torch.zeros(num_classes)\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        output_cls = output == cls\n",
    "        target_cls = target == cls\n",
    "\n",
    "        intersection[cls] = torch.sum(output_cls & target_cls)\n",
    "        union[cls] = torch.sum(output_cls | target_cls)\n",
    "        target_area[cls] = torch.sum(target_cls)\n",
    "\n",
    "    return intersection, union, target_area\n",
    "\n",
    "def batch_intersectionAndUnionGPU(logits: torch.Tensor, target: torch.Tensor, num_classes: int, ignore_index=255) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Calculate intersection and union for batch of tasks and shots.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): Predicted logits tensor.\n",
    "        target (torch.Tensor): Ground truth tensor.\n",
    "        num_classes (int): Number of classes.\n",
    "        ignore_index (int, optional): Index to ignore in evaluation. Defaults to 255.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Intersection, Union, Target area for each task and shot.\n",
    "    \"\"\"\n",
    "    n_task, shots, _, h, w = logits.size()\n",
    "    H, W = target.size()[-2:]\n",
    "\n",
    "    logits = F.interpolate(logits.view(n_task * shots, num_classes, h, w), size=(H, W), mode='bilinear', align_corners=True).view(n_task, shots, num_classes, H, W)\n",
    "    preds = logits.argmax(2)  # [n_task, shot, H, W]\n",
    "\n",
    "    area_intersection = torch.zeros(n_task, shots, num_classes)\n",
    "    area_union = torch.zeros(n_task, shots, num_classes)\n",
    "    area_target = torch.zeros(n_task, shots, num_classes)\n",
    "\n",
    "    for task in range(n_task):\n",
    "        for shot in range(shots):\n",
    "            i, u, t = intersectionAndUnion(preds[task][shot], target[task][shot], num_classes, ignore_index=ignore_index)\n",
    "            area_intersection[task, shot, :] = i\n",
    "            area_union[task, shot, :] = u\n",
    "            area_target[task, shot, :] = t\n",
    "\n",
    "    return area_intersection, area_union, area_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Classifier(object):\n",
    "    def __init__(self):\n",
    "        self.num_classes = 2\n",
    "        self.temperature = 20\n",
    "        self.adapt_iter = 50\n",
    "        self.weights = [1.0, 'auto', 'auto']\n",
    "        self.lr = 0.025\n",
    "        self.FB_param_update = [10]\n",
    "        self.visdom_freq = 5\n",
    "        self.FB_param_type = 'soft'\n",
    "        self.FB_param_noise = 0\n",
    "\n",
    "    def init_prototypes(self, features_s: torch.tensor, features_q: torch.tensor,\n",
    "                        gt_s: torch.tensor, gt_q: torch.tensor, subcls: List[int],\n",
    "                        callback) -> None:\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "            features_s : shape [n_task, shot, c, h, w]\n",
    "            features_q : shape [n_task, 1, c, h, w]\n",
    "            gt_s : shape [n_task, shot, H, W]\n",
    "            gt_q : shape [n_task, 1, H, W]\n",
    "\n",
    "        returns :\n",
    "            prototypes : shape [n_task, c]\n",
    "            bias : shape [n_task]\n",
    "        \"\"\"\n",
    "\n",
    "        # DownSample support masks\n",
    "        n_task, shot, c, h, w = features_s.size()\n",
    "        ds_gt_s = F.interpolate(gt_s.float(), size=features_s.shape[-2:], mode='nearest')\n",
    "        ds_gt_s = ds_gt_s.long().unsqueeze(2)  # [n_task, shot, 1, h, w]\n",
    "\n",
    "        # Computing prototypes\n",
    "        fg_mask = (ds_gt_s == 1)\n",
    "        fg_prototype = (features_s * fg_mask).sum(dim=(1, 3, 4))\n",
    "        fg_prototype /= (fg_mask.sum(dim=(1, 3, 4)) + 1e-10)  # [n_task, c]\n",
    "        self.prototype = fg_prototype\n",
    "\n",
    "        logits_q = self.get_logits(features_q)  # [n_tasks, shot, h, w]\n",
    "        self.bias = logits_q.mean(dim=(1, 2, 3))\n",
    "\n",
    "        assert self.prototype.size() == (n_task, c), self.prototype.size()\n",
    "        assert torch.isnan(self.prototype).sum() == 0, self.prototype\n",
    "\n",
    "        if callback is not None:\n",
    "            self.update_callback(callback, 0, features_s, features_q, subcls, gt_s, gt_q)\n",
    "\n",
    "    def get_logits(self, features: torch.tensor) -> torch.tensor:\n",
    "\n",
    "        \"\"\"\n",
    "        Computes the cosine similarity between self.prototype and given features\n",
    "        inputs:\n",
    "            features : shape [n_tasks, shot, c, h, w]\n",
    "\n",
    "        returns :\n",
    "            logits : shape [n_tasks, shot, h, w]\n",
    "        \"\"\"\n",
    "\n",
    "        # Put prototypes and features in the right shape for multiplication\n",
    "        features = features.permute((0, 1, 3, 4, 2))  # [n_task, shot, h, w, c]\n",
    "        prototype = self.prototype.unsqueeze(1).unsqueeze(2)  # [n_tasks, 1, 1, c]\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        print('feature', features.size(), 'prototype', prototype.unsqueeze(4).size())\n",
    "        print(\"features.matmul(prototype.unsqueeze(4)\", features.matmul(prototype.unsqueeze(4)).shape)\n",
    "        cossim = features.matmul(prototype.unsqueeze(4)).squeeze(4)  # [n_task, shot, h, w]\n",
    "        cossim /= ((prototype.unsqueeze(3).norm(dim=4) * \\\n",
    "                    features.norm(dim=4)) + 1e-10)  # [n_tasks, shot, h, w]\n",
    "\n",
    "        return self.temperature * cossim\n",
    "\n",
    "    def get_probas(self, logits: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "            logits : shape [n_tasks, shot, h, w]\n",
    "\n",
    "        returns :\n",
    "            probas : shape [n_tasks, shot, num_classes, h, w]\n",
    "        \"\"\"\n",
    "        logits_fg = logits - self.bias.unsqueeze(1).unsqueeze(2).unsqueeze(3)  # [n_tasks, shot, h, w]\n",
    "        probas_fg = torch.sigmoid(logits_fg).unsqueeze(2)\n",
    "        probas_bg = 1 - probas_fg\n",
    "        probas = torch.cat([probas_bg, probas_fg], dim=2)\n",
    "        return probas\n",
    "\n",
    "    def compute_FB_param(self, features_q: torch.tensor, gt_q: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "            features_q : shape [n_tasks, shot, c, h, w]\n",
    "            gt_q : shape [n_tasks, shot, h, w]\n",
    "\n",
    "        updates :\n",
    "             self.FB_param : shape [n_tasks, num_classes]\n",
    "        \"\"\"\n",
    "        ds_gt_q = F.interpolate(gt_q.float(), size=features_q.size()[-2:], mode='nearest').long()\n",
    "        valid_pixels = (ds_gt_q != 255).unsqueeze(2)  # [n_tasks, shot, num_classes, h, w]\n",
    "        assert (valid_pixels.sum(dim=(1, 2, 3, 4)) == 0).sum() == 0, valid_pixels.sum(dim=(1, 2, 3, 4))\n",
    "\n",
    "        one_hot_gt_q = to_one_hot(ds_gt_q, self.num_classes)  # [n_tasks, shot, num_classes, h, w]\n",
    "\n",
    "        oracle_FB_param = (valid_pixels * one_hot_gt_q).sum(dim=(1, 3, 4)) / valid_pixels.sum(dim=(1, 3, 4))\n",
    "        logits_q = self.get_logits(features_q)\n",
    "        probas = self.get_probas(logits_q).detach()\n",
    "        self.FB_param = (valid_pixels * probas).sum(dim=(1, 3, 4))\n",
    "        self.FB_param /= valid_pixels.sum(dim=(1, 3, 4))\n",
    "\n",
    "        # Compute the relative error\n",
    "        deltas = self.FB_param[:, 1] / oracle_FB_param[:, 1] - 1\n",
    "        return deltas\n",
    "\n",
    "    def get_entropies(self,\n",
    "                      valid_pixels: torch.tensor,\n",
    "                      probas: torch.tensor,\n",
    "                      reduction='sum') -> Tuple[torch.tensor, torch.tensor, torch.tensor]:\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "            probas : shape [n_tasks, shot, num_class, h, w]\n",
    "            valid_pixels: shape [n_tasks, shot, h, w]\n",
    "\n",
    "        returns:\n",
    "            d_kl : FB proportion kl [n_tasks,]\n",
    "            cond_entropy : Entropy of predictions [n_tasks,]\n",
    "            marginal : Current marginal distribution over labels [n_tasks, num_classes]\n",
    "        \"\"\"\n",
    "        n_tasks, shot, num_classes, h, w = probas.size()\n",
    "        assert (valid_pixels.sum(dim=(1, 2, 3)) == 0).sum() == 0, \\\n",
    "               (valid_pixels.sum(dim=(1, 2, 3)) == 0).sum()  # Make sure all tasks have a least 1 valid pixel\n",
    "\n",
    "        cond_entropy = - ((valid_pixels.unsqueeze(2) * (probas * torch.log(probas + 1e-10))).sum(2))\n",
    "        cond_entropy = cond_entropy.sum(dim=(1, 2, 3))\n",
    "        cond_entropy /= valid_pixels.sum(dim=(1, 2, 3))\n",
    "\n",
    "        marginal = (valid_pixels.unsqueeze(2) * probas).sum(dim=(1, 3, 4))\n",
    "        marginal /= valid_pixels.sum(dim=(1, 2, 3)).unsqueeze(1)\n",
    "\n",
    "        d_kl = (marginal * torch.log(marginal / (self.FB_param + 1e-10))).sum(1)\n",
    "\n",
    "        if reduction == 'sum':\n",
    "            cond_entropy = cond_entropy.sum(0)\n",
    "            d_kl = d_kl.sum(0)\n",
    "            assert not torch.isnan(cond_entropy), cond_entropy\n",
    "            assert not torch.isnan(d_kl), d_kl\n",
    "        elif reduction == 'mean':\n",
    "            cond_entropy = cond_entropy.mean(0)\n",
    "            d_kl = d_kl.mean(0)\n",
    "        return d_kl, cond_entropy, marginal\n",
    "\n",
    "    def get_ce(self,\n",
    "               probas: torch.tensor,\n",
    "               valid_pixels: torch.tensor,\n",
    "               one_hot_gt: torch.tensor,\n",
    "               reduction: str = 'sum') -> torch.tensor:\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "            probas : shape [n_tasks, shot, c, h, w]\n",
    "            one_hot_gt: shape [n_tasks, shot, num_classes, h, w]\n",
    "            valid_pixels : shape [n_tasks, shot, h, w]\n",
    "\n",
    "        updates :\n",
    "             ce : Cross-Entropy between one_hot_gt and probas, shape [n_tasks,]\n",
    "        \"\"\"\n",
    "        ce = - ((valid_pixels.unsqueeze(2) * (one_hot_gt * torch.log(probas + 1e-10))).sum(2))  # [n_tasks, shot, h, w]\n",
    "        ce = ce.sum(dim=(1, 2, 3))  # [n_tasks]\n",
    "        ce /= valid_pixels.sum(dim=(1, 2, 3))\n",
    "        if reduction == 'sum':\n",
    "            ce = ce.sum(0)\n",
    "        elif reduction == 'mean':\n",
    "            ce = ce.mean(0)\n",
    "        return ce\n",
    "\n",
    "    def RePRI(self,\n",
    "              features_s: torch.tensor,\n",
    "              features_q: torch.tensor,\n",
    "              gt_s: torch.tensor,\n",
    "              gt_q: torch.tensor,\n",
    "              subcls: List,\n",
    "              n_shots: torch.tensor,\n",
    "              callback) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Performs RePRI inference\n",
    "\n",
    "        inputs:\n",
    "            features_s : shape [n_tasks, shot, c, h, w]\n",
    "            features_q : shape [n_tasks, shot, c, h, w]\n",
    "            gt_s : shape [n_tasks, shot, h, w]\n",
    "            gt_q : shape [n_tasks, shot, h, w]\n",
    "            subcls : List of classes present in each task\n",
    "            n_shots : # of support shots for each task, shape [n_tasks,]\n",
    "\n",
    "        updates :\n",
    "            prototypes : torch.Tensor of shape [n_tasks, num_class, c]\n",
    "\n",
    "        returns :\n",
    "            deltas : Relative error on FB estimation right after first update, for each task,\n",
    "                     shape [n_tasks,]\n",
    "        \"\"\"\n",
    "        deltas = torch.zeros_like(n_shots)\n",
    "        l1, l2, l3 = self.weights\n",
    "        if l2 == 'auto':\n",
    "            l2 = 1 / n_shots\n",
    "        else:\n",
    "            l2 = l2 * torch.ones_like(n_shots)\n",
    "        if l3 == 'auto':\n",
    "            l3 = 1 / n_shots\n",
    "        else:\n",
    "            l3 = l3 * torch.ones_like(n_shots)\n",
    "\n",
    "        self.prototype.requires_grad_()\n",
    "        self.bias.requires_grad_()\n",
    "        optimizer = torch.optim.SGD([self.prototype, self.bias], lr=self.lr)\n",
    "\n",
    "        ds_gt_q = F.interpolate(gt_q.float(), size=features_s.size()[-2:], mode='nearest').long()\n",
    "        ds_gt_s = F.interpolate(gt_s.float(), size=features_s.size()[-2:], mode='nearest').long()\n",
    "\n",
    "        valid_pixels_q = (ds_gt_q != 255).float()  # [n_tasks, shot, h, w]\n",
    "        valid_pixels_s = (ds_gt_s != 255).float()  # [n_tasks, shot, h, w]\n",
    "\n",
    "        one_hot_gt_s = to_one_hot(ds_gt_s, self.num_classes)  # [n_tasks, shot, num_classes, h, w]\n",
    "\n",
    "        for iteration in range(1, self.adapt_iter):\n",
    "\n",
    "            logits_s = self.get_logits(features_s)  # [n_tasks, shot, num_class, h, w]\n",
    "            logits_q = self.get_logits(features_q)  # [n_tasks, 1, num_class, h, w]\n",
    "            proba_q = self.get_probas(logits_q)\n",
    "            proba_s = self.get_probas(logits_s)\n",
    "\n",
    "            d_kl, cond_entropy, marginal = self.get_entropies(valid_pixels_q,\n",
    "                                                              proba_q,\n",
    "                                                              reduction='none')\n",
    "            ce = self.get_ce(proba_s, valid_pixels_s, one_hot_gt_s, reduction='none')\n",
    "            loss = l1 * ce + l2 * d_kl + l3 * cond_entropy\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.sum(0).backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update FB_param\n",
    "            if (iteration + 1) in self.FB_param_update  \\\n",
    "                    and ('oracle' not in self.FB_param_type) and (l2.sum().item() != 0):\n",
    "                deltas = self.compute_FB_param(features_q, gt_q).cpu()\n",
    "                l2 += 1\n",
    "\n",
    "            if callback is not None and (iteration + 1) % self.visdom_freq == 0:\n",
    "                self.update_callback(callback, iteration, features_s, features_q, subcls, gt_s, gt_q)\n",
    "        return deltas\n",
    "\n",
    "    def get_mIoU(self,\n",
    "                 probas: torch.tensor,\n",
    "                 gt: torch.tensor,\n",
    "                 subcls: torch.tensor,\n",
    "                 reduction: str = 'mean') -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Computes the mIoU over the current batch of tasks being processed\n",
    "\n",
    "        inputs:\n",
    "            probas : shape [n_tasks, shot, num_class, h, w]\n",
    "            gt : shape [n_tasks, shot, h, w]\n",
    "            subcls : List of classes present in each task\n",
    "\n",
    "\n",
    "        returns :\n",
    "            class_IoU : Classwise IoU (or mean of it), shape\n",
    "        \"\"\"\n",
    "        intersection, union, _ = batch_intersectionAndUnionGPU(probas, gt, self.num_classes)  # [num_tasks, shot, num_class]\n",
    "        inter_count = defaultdict(int)\n",
    "        union_count = defaultdict(int)\n",
    "\n",
    "        for i, classes_ in enumerate(subcls):\n",
    "            inter_count[0] += intersection[i, 0, 0]\n",
    "            union_count[0] += union[i, 0, 0]\n",
    "            for j, class_ in enumerate(classes_):\n",
    "                inter_count[class_] += intersection[i, 0, j + 1]  # Do not count background\n",
    "                union_count[class_] += union[i, 0, j + 1]\n",
    "        class_IoU = torch.tensor([inter_count[subcls] / union_count[subcls] for subcls in inter_count if subcls != 0])\n",
    "        if reduction == 'mean':\n",
    "            return class_IoU.mean()\n",
    "        elif reduction == 'none':\n",
    "            return class_IoU\n",
    "\n",
    "    # def update_callback(self, callback, iteration: int, features_s: torch.tensor,\n",
    "    #                     features_q: torch.tensor, subcls: List[int],\n",
    "    #                     gt_s: torch.tensor, gt_q: torch.tensor) -> None:\n",
    "    #     \"\"\"\n",
    "    #     Updates the visdom callback in case live visualization of metrics is desired\n",
    "\n",
    "    #     inputs:\n",
    "    #         iteration: Current inference iteration\n",
    "    #         features_s : shape [n_tasks, shot, c, h, w]\n",
    "    #         features_q : shape [n_tasks, shot, c, h, w]\n",
    "    #         gt_s : shape [n_tasks, shot, h, w]\n",
    "    #         gt_q : shape [n_tasks, shot, h, w]\n",
    "    #         subcls : List of classes present in each task\n",
    "\n",
    "\n",
    "    #     returns :\n",
    "    #         callback : Visdom logger\n",
    "    #     \"\"\"\n",
    "    #     logits_q = self.get_logits(features_q)  # [n_tasks, shot, num_class, h, w]\n",
    "    #     logits_s = self.get_logits(features_s)  # [n_tasks, shot, num_class, h, w]\n",
    "    #     proba_q = self.get_probas(logits_q).detach()  # [n_tasks, shot, num_class, h, w]\n",
    "    #     proba_s = self.get_probas(logits_s).detach()  # [n_tasks, shot, num_class, h, w]\n",
    "\n",
    "    #     f_resolution = features_s.size()[-2:]\n",
    "    #     ds_gt_q = F.interpolate(gt_q.float(), size=f_resolution, mode='nearest').long()\n",
    "        # ds_gt_s = F.interpolate(gt_s.float(), size=f_resolution, mode='nearest').long()\n",
    "\n",
    "        # valid_pixels_q = (ds_gt_q != 255).float()  # [n_tasks, shot, h, w]\n",
    "        # valid_pixels_s = (ds_gt_s != 255).float()  # [n_tasks, shot, h, w]\n",
    "\n",
    "        # one_hot_gt_q = to_one_hot(ds_gt_q, self.num_classes)  # [n_tasks, shot, num_classes, h, w]\n",
    "        # oracle_FB_param = (valid_pixels_q.unsqueeze(2) * one_hot_gt_q).sum(dim=(1, 3, 4))\n",
    "        # oracle_FB_param /= (valid_pixels_q.unsqueeze(2)).sum(dim=(1, 3, 4))\n",
    "\n",
    "        # one_hot_gt_s = to_one_hot(ds_gt_s, self.num_classes)  # [n_tasks, shot, num_classes, h, w]\n",
    "        # ce_s = self.get_ce(proba_s, valid_pixels_s, one_hot_gt_s)\n",
    "        # ce_q = self.get_ce(proba_q, valid_pixels_q, one_hot_gt_q)\n",
    "\n",
    "        # mIoU_q = self.get_mIoU(proba_q, gt_q, subcls)\n",
    "\n",
    "        # callback.scalar('mIoU_q', iteration, mIoU_q, title='mIoU')\n",
    "        # if iteration > 0:\n",
    "        #     d_kl, cond_entropy, marginal = self.get_entropies(valid_pixels_q,\n",
    "        #                                                       proba_q,\n",
    "        #                                                       reduction='mean')\n",
    "        #     marginal2oracle = (oracle_FB_param * torch.log(oracle_FB_param / marginal + 1e-10)).sum(1).mean()\n",
    "        #     FB_param2oracle = (oracle_FB_param * torch.log(oracle_FB_param / self.FB_param + 1e-10)).sum(1).mean()\n",
    "        #     callback.scalars(['Cond', 'marginal2oracle', 'FB_param2oracle'], iteration,\n",
    "        #                      [cond_entropy, marginal2oracle, FB_param2oracle], title='Entropy')\n",
    "        # callback.scalars(['ce_s', 'ce_q'], iteration, [ce_s, ce_q], title='CE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch\n",
    "\n",
    "\n",
    "# ==================================================================================================\n",
    "# Taken from https://github.com/hszhao/semseg/blob/master/model/resnet.py ==========================\n",
    "# ==================================================================================================\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, deep_base=True):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.deep_base = deep_base\n",
    "        if not self.deep_base:\n",
    "            self.inplanes = 64\n",
    "            self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(64)\n",
    "        else:\n",
    "            self.inplanes = 128\n",
    "            self.conv1 = conv3x3(3, 64, stride=2)\n",
    "            self.bn1 = nn.BatchNorm2d(64)\n",
    "            self.conv2 = conv3x3(64, 64)\n",
    "            self.bn2 = nn.BatchNorm2d(64)\n",
    "            self.conv3 = conv3x3(64, 128)\n",
    "            self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        if self.deep_base:\n",
    "            x = self.relu(self.bn2(self.conv2(x)))\n",
    "            x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet50(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        # model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
    "        model_path = './initmodel/resnet50_v2.pth'\n",
    "        model.load_state_dict(torch.load(model_path), strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.randn(1, 3, 224, 224)\n",
    "out = model(inp)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature torch.Size([2, 1, 7, 7, 2048]) prototype torch.Size([2, 1, 1, 2048, 1])\n",
      "features.matmul(prototype.unsqueeze(4) torch.Size([2, 1, 7, 7, 1])\n",
      "feature torch.Size([2, 1, 7, 7, 2048]) prototype torch.Size([2, 1, 1, 2048, 1])\n",
      "features.matmul(prototype.unsqueeze(4) torch.Size([2, 1, 7, 7, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 2048, 7, 7]),\n",
       " torch.Size([2, 1, 2048, 7, 7]),\n",
       " torch.Size([2, 2048]),\n",
       " torch.Size([2, 2]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = Classifier()\n",
    "batch_size_val, shot, c, h, w = 2, 5, 2048, 7, 7\n",
    "image_size = 224\n",
    "gt_s = 255 * torch.rand(batch_size_val, shot, image_size,\n",
    "                                    image_size).long()\n",
    "gt_q = 255 * torch.rand(batch_size_val, 1, image_size,\n",
    "                        image_size).long()\n",
    "n_shots = torch.rand(batch_size_val)\n",
    "features_s = torch.rand(batch_size_val, shot, c, h, w)\n",
    "features_q = torch.rand(batch_size_val, 1, c, h, w)\n",
    "classifier.init_prototypes(features_s, features_q, gt_s, gt_q, [1, 2], None)\n",
    "batch_deltas = classifier.compute_FB_param(features_q=features_q, gt_q=gt_q)\n",
    "features_s.shape, features_q.shape, classifier.prototype.shape, classifier.FB_param.shape\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
