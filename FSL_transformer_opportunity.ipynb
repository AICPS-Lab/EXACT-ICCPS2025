{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read each file in datasets/Opportunity/by_subject:\n",
    "\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "directory = 'datasets/Opportunity/by_subject'\n",
    "csv_files = [file for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "\n",
    "for file in csv_files:\n",
    "    # print(df.columns)\n",
    "    file_path = os.path.join(directory, file)\n",
    "    # Process the CSV file as needed\n",
    "    # For example, you can use pandas to read the CSV file:\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # print(df.to_numpy()[0:6])\n",
    "    # print(df.to_numpy()[:, 9])\n",
    "    \n",
    "    # data statistics in plt:\n",
    "    print(np.unique(df['ML_Both_Arms']))\n",
    "    df['ML_Both_Arms'].plot(kind='hist', bins=18)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380\n"
     ]
    }
   ],
   "source": [
    "# read each file in datasets/Opportunity/by_subject:\n",
    "\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "directory = 'raw_datasets/SPAR9X'\n",
    "csv_files = [file for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "\n",
    "for file in csv_files:\n",
    "    # print(df.columns)\n",
    "    file_path = os.path.join(directory, file)\n",
    "    # Process the CSV file as needed\n",
    "    # For example, you can use pandas to read the CSV file:\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # print(df.to_numpy()[0:6])\n",
    "    # print(df.to_numpy()[:, 9])\n",
    "print(len(csv_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1306)\n",
      "tensor(1643)\n",
      "tensor(1244)\n",
      "tensor(1239)\n",
      "tensor(1228)\n",
      "tensor(1551)\n",
      "tensor(1597)\n",
      "tensor(1527)\n",
      "tensor(1858)\n",
      "tensor(1406)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from utilities import *\n",
    "sw = sliding_windows(100, 50)\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "\n",
    "def replace_with_most_frequent(y):\n",
    "    #NOTE: func used for OPPORTUNITY dataset by subject, if there are more than 1 label in a window, replace with the most frequent label\n",
    "    # Iterate over each sample\n",
    "    for i in range(y.size(0)):\n",
    "        # Find unique labels and their counts\n",
    "        unique_labels, counts = torch.unique(y[i], sorted=True, return_counts=True)\n",
    "        # print(unique_labels, counts)\n",
    "        if  len(unique_labels) > 1:\n",
    "            # Find the most frequent label, exclude 0:\n",
    "            most_frequent_label = unique_labels[1:][torch.argmax(counts[1:])]\n",
    "            y[i] = most_frequent_label\n",
    "    return y\n",
    "\n",
    "\n",
    "for file in csv_files:\n",
    "    # print(df.columns)\n",
    "    file_path = os.path.join(directory, file)\n",
    "    # Process the CSV file as needed\n",
    "    # For example, you can use pandas to read the CSV file:\n",
    "    df = pd.read_csv(file_path)\n",
    "    # print(df.to_numpy().shape)\n",
    "    x, _ = sw(torch.tensor(df.to_numpy()[:, [0,1,2,6,7,8]]), torch.tensor(df.to_numpy())) # sec para as placeholder, not used\n",
    "    # divide the gravity by 9.81 on 0-2: \n",
    "    x[:, :, 0:3] = x[:, :, 0:3] / 9.81\n",
    "    y = int(file.split('_')[1][1::]) - 1\n",
    "    X.append(x)\n",
    "    Y.extend([y] * x.size(0))\n",
    "\n",
    "X = torch.cat(X, dim=0)\n",
    "# Y = torch.cat(Y, dim=0)\n",
    "Y = torch.tensor(Y)\n",
    "X.shape, Y.shape\n",
    "\n",
    "for i in range(10):\n",
    "    print(torch.sum(Y == i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted with mean: [-0.01048841 -0.51112092  0.12738925 -0.00062111  0.00209199 -0.00385441], and std: [0.54560239 0.56300103 0.39417075 0.64261371 0.7944528  0.92907233]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "class StandardTransform(torch.nn.Module):\n",
    "    def __init__(self, scaler='standard'):\n",
    "        super(StandardTransform, self).__init__()\n",
    "        if scaler == 'standard':\n",
    "            self.scaler = StandardScaler()\n",
    "        else:\n",
    "            raise NotImplementedError('Only standard scaler is implemented')\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        data = self.scaler.transform(data)\n",
    "        return torch.tensor(data)\n",
    "    \n",
    "    def fit(self, data):\n",
    "        n_samples, n_time_steps, n_features = data.shape\n",
    "        data_reshaped = data.reshape(-1, n_features)  # The shape becomes (n_samples * n_time_steps, n_features)\n",
    "        self.scaler.fit(data_reshaped)\n",
    "        print('Fitted with mean: {}, and std: {}'.format(self.scaler.mean_, np.sqrt(self.scaler.var_)))\n",
    "        return self\n",
    "\n",
    "\n",
    "class MultivariateTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, Y, transform=None):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.transform:\n",
    "            return self.transform(self.X[index]), self.Y[index]\n",
    "        return torch.tensor(self.X[index]), int(self.Y[index])\n",
    "\n",
    "# Dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into train, valid, test sets:\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "train_X, valid_X, train_Y, valid_Y = train_test_split(train_X, train_Y, test_size=0.2, random_state=42)\n",
    "# Initialize the StandardScaler\n",
    "ST = StandardTransform().fit(train_X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create dataloaders for train and test sets\n",
    "train_dataloader = DataLoader(MultivariateTimeSeriesDataset(train_X, train_Y, transform=ST), batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(MultivariateTimeSeriesDataset(valid_X, valid_Y, transform=ST), batch_size=32, shuffle=False)\n",
    "test_dataloader = DataLoader(MultivariateTimeSeriesDataset(test_X, test_Y, transform=ST), batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from utilities import *\n",
    "from torch.nn.modules.transformer import TransformerEncoder, TransformerEncoderLayer\n",
    "num_epochs = 100\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=6, hidden_size=1024, num_layers=2, output_size=18):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "import torch.nn.functional as F\n",
    "import math \n",
    "\n",
    "\n",
    "    \n",
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, ntoken=6, ninp=64, nhead=1, nhid=128, nlayers=6, dropout=0.1, activation='relu'):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_emb = nn.Linear(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.relu = nn.ReLU()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=ninp, nhead=nhead, dim_feedforward=nhid, dropout=dropout, activation=activation, batch_first=True)\n",
    "        encoder_norm = nn.LayerNorm(ninp)   \n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, nlayers, norm=encoder_norm)\n",
    "        self.decoder = nn.Linear(ninp, 1)\n",
    "        # max layer:\n",
    "        self.max = nn.MaxPool1d(100)\n",
    "        self.psi = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.input_emb(src)\n",
    "        src = self.relu(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        \n",
    "        # max layer:\n",
    "        # output = output.transpose(0, 1)\n",
    "        # output = self.max(output)\n",
    "        # output = output.transpose(1, 2)\n",
    "        output = self.decoder(output)\n",
    "        output = output.squeeze(-1)\n",
    "        output = self.psi(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "        # return F.sigmoid(output).squeeze(-1) # return F.log_softmax(output, dim=-1)\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the CNN model\n",
    "model = TransformerModel().to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "liveloss = PlotLosses()\n",
    "# Iterate over the training data\n",
    "logs = {}\n",
    "# change the plt size:\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_dataloader):\n",
    "        # Forward pass\n",
    "        images = images.float().to(device)\n",
    "        labels = labels.squeeze().long().to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # train loss and accuracy check:\n",
    "        if i % 1000 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = labels.size(0)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            # print('Accuracy: {:.2f}%'.format(correct / total * 100))\n",
    "            logs['loss'] = loss.item()\n",
    "            logs['accuracy'] = correct / total * 100\n",
    "    \n",
    "            # validation loss and accuracy check:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            losses =  []\n",
    "            for images, labels in val_dataloader:\n",
    "                images = images.float().to(device)\n",
    "                labels = labels.squeeze().long().to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                losses.append(criterion(outputs, labels).item())\n",
    "            logs['val_loss'] = np.mean(losses)\n",
    "            logs['val_accuracy'] = correct / total * 100\n",
    "    liveloss.update(logs)\n",
    "    liveloss.send()\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_dataloader:\n",
    "        images = images.float().to(device)\n",
    "        labels = labels.squeeze().long().to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print('Test Accuracy: {:.2f}%'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './saved_model/transformer_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidwang/opt/anaconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'criterion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m----> 8\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m(outputs, labels)\n\u001b[1;32m      9\u001b[0m reshaped_images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6\u001b[39m)\n\u001b[1;32m     10\u001b[0m pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'criterion' is not defined"
     ]
    }
   ],
   "source": [
    "model = TransformerModel().to(device)\n",
    "model.load_state_dict(torch.load('./saved_model/transformer_model.pth'))\n",
    "for i, (images, labels) in enumerate(train_dataloader):\n",
    "    # Forward pass\n",
    "    images = images.float().to(device)\n",
    "    labels = labels.squeeze().long().to(device)\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    reshaped_images = images.view(-1, 6)\n",
    "    pred = torch.argmax(outputs, dim=1)\n",
    "\n",
    "    plt.plot(reshaped_images.cpu().numpy(), color='black')\n",
    "    # for each pred, repeat it for 50 times: \n",
    "    plt.plot(np.repeat(pred.cpu().numpy(), 50), color='green')\n",
    "    # for each label, do the same:\n",
    "    plt.plot(np.repeat(labels.cpu().numpy(), 50), color='red')\n",
    "    plt.show()\n",
    "    break\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install easyfsl\n",
    "from easyfsl.samplers import TaskSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrototypicalNetworks(nn.Module):\n",
    "    def __init__(self, backbone: nn.Module):\n",
    "        super(PrototypicalNetworks, self).__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        support_images: torch.Tensor,\n",
    "        support_labels: torch.Tensor,\n",
    "        query_images: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predict query labels using labeled support images.\n",
    "        \"\"\"\n",
    "        # Extract the features of support and query images\n",
    "        z_support = self.backbone.forward(support_images)\n",
    "        z_query = self.backbone.forward(query_images)\n",
    "\n",
    "        # Infer the number of different classes from the labels of the support set\n",
    "        n_way = len(torch.unique(support_labels))\n",
    "        # Prototype i is the mean of all instances of features corresponding to labels == i\n",
    "        z_proto = torch.cat(\n",
    "            [\n",
    "                z_support[torch.nonzero(support_labels == label)].mean(0)\n",
    "                for label in range(n_way)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Compute the euclidean distance from queries to prototypes\n",
    "        dists = torch.cdist(z_query, z_proto)\n",
    "\n",
    "        # And here is the super complicated operation to transform those distances into classification scores!\n",
    "        scores = -dists\n",
    "        return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidwang/opt/anaconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "backbone_model = TransformerModel().to(device)\n",
    "backbone_model.load_state_dict(torch.load('./saved_model/transformer_model.pth'))\n",
    "\n",
    "backbone_model.psi = nn.Flatten()\n",
    "prototype_model = PrototypicalNetworks(backbone_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WAY = 5  # Number of classes in a task\n",
    "N_SHOT = 5  # Number of images per class in the support set\n",
    "N_QUERY = 10  # Number of images per class in the query set\n",
    "N_EVALUATION_TASKS = 100\n",
    "test_set = MultivariateTimeSeriesDataset(test_X, test_Y, transform=ST)\n",
    "# check each class has what number of samples:\n",
    "# print(set(test_Y.tolist()))\n",
    "test_set.get_labels = lambda: test_set.Y.tolist()\n",
    "\n",
    "test_sampler = TaskSampler(\n",
    "    test_set, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_EVALUATION_TASKS\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_sampler=test_sampler,\n",
    "    # num_workers=0,\n",
    "    # pin_memory=False,\n",
    "    collate_fn=test_sampler.episodic_collate_fn,\n",
    ")\n",
    "# next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     example_support_images,\n",
    "#     example_support_labels,\n",
    "#     example_query_images,\n",
    "#     example_query_labels,\n",
    "#     example_class_ids,\n",
    "# ) = next(iter(test_loader))\n",
    "\n",
    "# prototype_model.eval()\n",
    "# example_scores = prototype_model(\n",
    "#     example_support_images.float().to(device),\n",
    "#     example_support_labels.float().to(device),\n",
    "#     example_query_images.float().to(device),\n",
    "# ).detach()\n",
    "\n",
    "# _, example_predicted_labels = torch.max(example_scores.data, 1)\n",
    "\n",
    "# print(\"Ground Truth / Predicted\")\n",
    "# for i in range(len(example_query_labels)):\n",
    "#     print(\n",
    "#         f\"{test_set.Y[example_class_ids[example_query_labels[i]]]} / {test_set.Y[example_class_ids[example_predicted_labels[i]]]}\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 35.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model tested on 100 tasks. Accuracy: 75.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_on_one_task(\n",
    "    support_images: torch.Tensor,\n",
    "    support_labels: torch.Tensor,\n",
    "    query_images: torch.Tensor,\n",
    "    query_labels: torch.Tensor,\n",
    ") -> [int, int]:\n",
    "    \"\"\"\n",
    "    Returns the number of correct predictions of query labels, and the total number of predictions.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        torch.max(\n",
    "            prototype_model(support_images.float().to(device), support_labels.float().to(device), query_images.float().to(device))\n",
    "            .detach()\n",
    "            .data,\n",
    "            1,\n",
    "        )[1]\n",
    "        == query_labels.float().to(device)\n",
    "    ).sum().item(), len(query_labels)\n",
    "\n",
    "\n",
    "def evaluate(data_loader: DataLoader):\n",
    "    # We'll count everything and compute the ratio at the end\n",
    "    total_predictions = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    # eval mode affects the behaviour of some layers (such as batch normalization or dropout)\n",
    "    # no_grad() tells torch not to keep in memory the whole computational graph (it's more lightweight this way)\n",
    "    prototype_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for episode_index, \\\n",
    "            (support_images, support_labels, query_images, query_labels, class_ids,)\\\n",
    "            in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "\n",
    "            correct, total = evaluate_on_one_task(\n",
    "                support_images, support_labels, query_images, query_labels\n",
    "            )\n",
    "\n",
    "            total_predictions += total\n",
    "            correct_predictions += correct\n",
    "\n",
    "    print(\n",
    "        f\"Model tested on {len(data_loader)} tasks. Accuracy: {(100 * correct_predictions/total_predictions):.2f}%\"\n",
    "    )\n",
    "\n",
    "\n",
    "evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
