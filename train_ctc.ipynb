{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from methods import Segmenter, TransformerModel, LSTM, CRNN, UNet,CCRNN, UNet2, PatchTST\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from methods.unetc import UNetc\n",
    "from methods.unetr import UNetrt\n",
    "from utilities import printc, seed\n",
    "from utils_loader import get_dataloaders, test_idea_dataloader_ABC_to_BCA, test_idea_dataloader_long_A_B_to_AB\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from utils_metrics import eval_dense_label_to_classification, mean_iou, visualize_softmax\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "from utilities import sliding_windows\n",
    "import torch\n",
    "\n",
    "def load_data():\n",
    "    filename = './datasets/spar/spar_dataset'\n",
    "    # read through the folder that end with csv:\n",
    "    sw = sliding_windows(300, 150)\n",
    "    def process(files, sw, le: LabelEncoder):\n",
    "        nps = []\n",
    "        labels = []\n",
    "        for file in files:\n",
    "            df = pd.read_csv(os.path.join(filename, file))\n",
    "            nps.append(df.to_numpy()[:, 1:7])\n",
    "            labels.append([le.transform([file.split('_')[1]])[0]]*df.shape[0])\n",
    "        nps = np.concatenate(nps, axis=0)\n",
    "        labels = np.concatenate(labels, axis=0)\n",
    "        return sw(torch.tensor(nps).float(), torch.tensor(labels))\n",
    "    \n",
    "    classes = []\n",
    "    subjs = []\n",
    "    for root, dirs, files in os.walk(filename):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\") and 'R' in file:\n",
    "                label = file.split('_')[1]\n",
    "                subj = file.split('_')[0]\n",
    "                if label not in classes:\n",
    "                    classes.append(label)\n",
    "                if subj not in subjs:\n",
    "                    subjs.append(subj)\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le.fit(classes)\n",
    "    \n",
    "    # read through the folder that end with csv:\n",
    "    samples = []\n",
    "    labels = []\n",
    "    for each_subj in subjs:\n",
    "        collection = {class_name: [] for class_name in classes}\n",
    "        for root, dirs, files in os.walk(filename):\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\") and 'R' in file and each_subj == file.split('_')[0]:\n",
    "                    label = file.split('_')[1]\n",
    "                    collection[label].append(file)\n",
    "        leftover = []\n",
    "        for key in collection:\n",
    "            # sort the files\n",
    "            sorted_files = sorted(collection[key], key=lambda x: int(x.split('_')[3].split('.')[0]))\n",
    "            sample, lab = process(sorted_files, sw, le)\n",
    "            samples.append(sample)\n",
    "            labels.append(lab)\n",
    "    samples = torch.cat(samples, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    return samples, labels\n",
    "    \n",
    "\n",
    "def load_data1():\n",
    "    filename = './datasets/spar/spar_dataset'\n",
    "    seed = 0\n",
    "    # read through the folder that end with csv:\n",
    "    def combo_to_np(combo, le: LabelEncoder):\n",
    "        samples = []\n",
    "        labels = []\n",
    "        for each_combo in combo:\n",
    "            for each_file in each_combo:\n",
    "                df = pd.read_csv(os.path.join(filename, each_file))\n",
    "                samples.append(df.to_numpy()[:, 1:7])\n",
    "                transformed = le.transform([each_file.split('_')[1]])[0] + 1\n",
    "                labels.append([transformed] * df.shape[0])\n",
    "                # print('labels', labels)\n",
    "        # convert to np:\n",
    "        samples = np.concatenate(samples)\n",
    "        labels = np.concatenate(labels)\n",
    "        # print(samples.shape, labels.shape)\n",
    "        return samples, labels\n",
    "    \n",
    "    if True:\n",
    "        filename = './datasets/spar/spar_dataset'\n",
    "        seed = 0\n",
    "        # read through the folder that end with csv:\n",
    "        classes = []\n",
    "        subjs = []\n",
    "        for root, dirs, files in os.walk(filename):\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\") and 'R' in file:\n",
    "                    label = file.split('_')[1]\n",
    "                    subj = file.split('_')[0]\n",
    "                    if label not in classes:\n",
    "                        classes.append(label)\n",
    "                    if subj not in subjs:\n",
    "                        subjs.append(subj)\n",
    "                    # print(os.path.join(root, file))\n",
    "                    # df = pd.read_csv(os.path.join(root, file))\n",
    "\n",
    "        # generate train combo => ABC, ABD, ... # test combo => BCD (some combo that was not seem in train) based on the classes\n",
    "        train_combo = [('E1', 'E2', 'E3')]\n",
    "        test_combo = [('E3', 'E2', 'E1')]\n",
    "\n",
    "        assert all([each in classes for comb in train_combo for each in comb]), 'train combo not in classes'\n",
    "        assert all([each in classes for comb in test_combo for each in comb]), 'test combo not in classes'\n",
    "        train_classes = list(set([each for comb in train_combo for each in comb]))\n",
    "        le = LabelEncoder()\n",
    "        le.fit(train_classes)\n",
    "        random.seed(seed)\n",
    "        train_samples = []\n",
    "        test_samples = []\n",
    "        for each_subj in subjs:\n",
    "            collection = {class_name: [] for class_name in classes}\n",
    "            for root, dirs, files in os.walk(filename):\n",
    "                for file in files:\n",
    "                    if file.endswith(\".csv\") and 'R' in file and each_subj == file.split('_')[0]:\n",
    "                        label = file.split('_')[1]\n",
    "                        collection[label].append(file)\n",
    "\n",
    "            lengths = [len(collection[class_name]) for class_name in classes]\n",
    "            minimum = min(lengths)\n",
    "            # 80% for train, 20% for test\n",
    "            train_length = int(minimum * 0.8)\n",
    "            test_length = minimum - train_length\n",
    "            # print(f'{each_subj} has {minimum} samples, {train_length} for train, {test_length} for test')\n",
    "            # generate train and test data\n",
    "            \n",
    "            # randomly extract one sample of label from the collection and remove that label from the list in the collection:\n",
    "            for _ in range(train_length):\n",
    "                for i in train_combo:\n",
    "                    cur_combo = []\n",
    "                    for j in i:\n",
    "                        selected_sample = random.choice(collection[j])\n",
    "                        collection[j].remove(selected_sample)\n",
    "                        cur_combo.append(selected_sample)\n",
    "                    train_samples.append(cur_combo)\n",
    "            for _ in range(test_length):\n",
    "                for i in test_combo:\n",
    "                    cur_combo = []\n",
    "                    for j in i:\n",
    "                        selected_sample = random.choice(collection[j])\n",
    "                        collection[j].remove(selected_sample)\n",
    "                        cur_combo.append(selected_sample)\n",
    "                    test_samples.append(cur_combo)\n",
    "        train_s = combo_to_np(train_samples, le)\n",
    "        test_s = combo_to_np(test_samples, le)\n",
    "    \n",
    "    sw = sliding_windows(300, 150)\n",
    "    \n",
    "    train_samples, train_labels = sw(torch.tensor(train_s[0]), torch.tensor(train_s[1]))\n",
    "    test_samples, test_labels = sw(torch.tensor(test_s[0]), torch.tensor(test_s[1]))\n",
    "    # Split the dataset into train, val and test:\n",
    "    train_samples, val_samples, train_labels, val_labels = train_test_split(train_samples, train_labels, test_size=0.2, random_state=42)\n",
    "    return train_samples, val_samples, test_samples, train_labels, val_labels, test_labels\n",
    "\n",
    "# samples, labels = load_data() \n",
    "train_samples, val_samples, test_samples, train_labels, val_labels, test_labels = load_data1()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset for ctc:\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, samples, labels):\n",
    "        self.samples = samples\n",
    "        self.labels = self._get_labels_ctc(labels)\n",
    "        self.sec_labels = labels\n",
    "    def _get_labels_ctc(self, labels):\n",
    "        # convert the labels to ctc format\n",
    "        # [1, 1, 1, 2, 2, 2, 3, 3, 3] -> [1, 2, 3], 3\n",
    "        lis = []\n",
    "        \n",
    "        # when the new label is different from the previous one, append the previous one to the list\n",
    "        for i in range(len(labels)):\n",
    "            cur_list = []\n",
    "            prev = labels[i, 0]\n",
    "            count = 1\n",
    "            for j in range(1, len(labels[i])):\n",
    "                if labels[i, j] == prev:\n",
    "                    if j == len(labels[i]) - 1:\n",
    "                        cur_list.append(prev)\n",
    "                else:\n",
    "                    cur_list.append(prev)\n",
    "                    prev = labels[i, j]\n",
    "                    count += 1\n",
    "            cur_list = torch.tensor(cur_list)\n",
    "            lis.append(cur_list)\n",
    "        return lis\n",
    "    @staticmethod\n",
    "    def custom_collate_fn(x):\n",
    "        batch = []\n",
    "        labels = []\n",
    "        lengths = []\n",
    "        for i in range(len(x)):\n",
    "            batch.append(x[i][0])\n",
    "            labels.extend(x[i][1])\n",
    "            lengths.append(len(x[i][1]))\n",
    "            # lengths.append()\n",
    "        batch = torch.stack(batch)\n",
    "        labels = torch.tensor(labels)\n",
    "        lengths = torch.tensor(lengths)\n",
    "        return batch, labels, lengths\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        # target, target_length\n",
    "        return self.samples[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 0.0349,  1.0595,  0.2088,  0.6428,  1.9648, -0.1645],\n",
      "        [ 0.0464,  0.9939,  0.1085,  0.6062,  1.9830, -0.1459],\n",
      "        [ 0.0426,  0.9146,  0.0546, -0.1829,  1.9559, -0.1445],\n",
      "        ...,\n",
      "        [ 0.2131, -0.0094, -0.5881, -0.4107,  3.2487, -0.3954],\n",
      "        [ 0.2324, -0.0292, -0.4669, -0.1402,  3.2083, -0.3127],\n",
      "        [ 0.3251, -0.0484, -0.4190,  0.1312,  3.1519, -0.2248]],\n",
      "       dtype=torch.float64), tensor([3, 1, 2]))\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(train_samples, train_labels)\n",
    "val_dataset = CustomDataset(val_samples, val_labels)\n",
    "test_dataset = CustomDataset(test_samples, test_labels)\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    print(train_dataset[i])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=CustomDataset.custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=CustomDataset.custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=CustomDataset.custom_collate_fn)\n",
    "\n",
    "# z = next(iter(train_loader))\n",
    "z = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm:\n",
    "\n",
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, num_classes)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9s/zgph6l3n6b75g5pycfr4d_n40000gn/T/ipykernel_39903/392500381.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_count = torch.tensor(labels_count).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [10/19], Loss: 32.4982\n",
      "Epoch [2/50], Step [10/19], Loss: 22.0050\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[164], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 24\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/research/lib/python3.9/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/research/lib/python3.9/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTM(6, 256, 2, 4).to(device)\n",
    "criterion = torch.nn.CTCLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(100):\n",
    "    for i, (images, labels, labels_count) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels_count = torch.tensor(labels_count).to(device)\n",
    "        # Forward pass\n",
    "        images = images.float()\n",
    "        outputs = model(images)\n",
    "        outputs = F.log_softmax(outputs, dim=2)\n",
    "        # print(outputs.shape, labels.shape, labels_count.shape)\n",
    "        input_lengths = torch.full((images.shape[0],), images.shape[1], dtype=torch.long)\n",
    "        loss = criterion(outputs.transpose(0, 1), labels,  input_lengths, labels_count)\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{50}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 3, 3, 3, 1, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 3, 4, 4, 4, 4, 3, 3, 3, 2, 2, 3, 3, 3, 3,\n",
      "        2, 2, 3, 3, 3, 2, 2, 3])\n",
      "tensor([2, 1, 3, 2])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9s/zgph6l3n6b75g5pycfr4d_n40000gn/T/ipykernel_39903/3252385248.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_count = torch.tensor(labels_count).to(device)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[162], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(labels[labels_count[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]: labels_count[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39mlabels_count[i]])\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_loader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39msec_labels[i])\n\u001b[0;32m---> 35\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(test_loader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39msec_labels[i])\n\u001b[1;32m     36\u001b[0m s \u001b[38;5;241m=\u001b[39m aggregate(predicted)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(s,)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def aggregate(output):\n",
    "    total = []\n",
    "    for i in range(len(output)):\n",
    "        prev = int(output[i][0])\n",
    "        for j in range(1, len(output[i])):\n",
    "            if output[i][j] != prev and output[i][j] != 0:\n",
    "                total.append(prev)\n",
    "                prev = int(output[i][j])\n",
    "        total.append(prev)\n",
    "    return total\n",
    "\n",
    "# test:\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels, labels_count in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels_count = torch.tensor(labels_count).to(device)\n",
    "        images = images.float()\n",
    "        outputs = model(images)\n",
    "        # print(outputs.shape)\n",
    "        # print(outputs)\n",
    "        outputs = F.softmax(outputs, dim=2)\n",
    "        input_lengths = torch.full((images.shape[0],), images.shape[1], dtype=torch.long)\n",
    "        loss = criterion(outputs.permute(1, 0, 2), labels,  input_lengths, labels_count)\n",
    "        _, predicted = torch.max(outputs.data, 2)\n",
    "        total += labels.size(0)\n",
    "        i = 1\n",
    "        print(predicted[i])\n",
    "        print(labels_count)\n",
    "        print(labels[labels_count[i-1]: labels_count[i-1]+labels_count[i]])\n",
    "        print(test_loader.dataset.sec_labels[i])\n",
    "        plt.plot(test_loader.dataset.sec_labels[i])\n",
    "        s = aggregate(predicted)\n",
    "        \n",
    "        print(s,)\n",
    "        print(labels.tolist())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
